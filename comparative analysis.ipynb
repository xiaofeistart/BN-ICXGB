{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "704f1b8d",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74638478",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-10T15:55:46.808974Z",
     "start_time": "2022-12-10T15:55:45.972099Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4a43cc3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-10T15:55:50.283039Z",
     "start_time": "2022-12-10T15:55:49.700171Z"
    }
   },
   "outputs": [],
   "source": [
    "feature=pd.read_csv('finalfeatures.csv')\n",
    "amount=pd.read_csv('费用类相关特征2.csv')\n",
    "amount=amount.set_index('个人编码')\n",
    "amount=amount[['总审批金额']]\n",
    "feature=feature.set_index('个人编码')\n",
    "feature=feature.drop(columns=['总审批金额'])\n",
    "feature=pd.concat([feature,amount],axis=1)\n",
    "feature = feature.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50f1101b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-10T15:55:50.941366Z",
     "start_time": "2022-12-10T15:55:50.798060Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Bayes Network features\n",
    "bayesdata=pd.read_csv('bayesdata2.csv')\n",
    "bayesdata=bayesdata.set_index('个人编码')\n",
    "feature=pd.concat([feature,bayesdata],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d023653",
   "metadata": {},
   "source": [
    "# BN-ICXGB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c65d16e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-10T15:55:59.575445Z",
     "start_time": "2022-12-10T15:55:59.487813Z"
    },
    "code_folding": [
     22
    ]
   },
   "outputs": [],
   "source": [
    "# cost-sensitive XGB\n",
    "import xgboost as xgb\n",
    "from scipy.special import expit\n",
    "\n",
    "class CSBoost:\n",
    "    def __init__(self, obj, lambda1=0, lambda2=0, learn_rate=0.01):\n",
    "        self.obj = obj\n",
    "        self.lambda1 = lambda1\n",
    "        self.lambda2 = lambda2\n",
    "        self.learn_rate=learn_rate\n",
    "        # alpha is l1, lambda is l2\n",
    "        params = {'random_state': 42, 'tree_method': 'exact', 'verbosity': 0, 'reg_alpha': lambda1,\n",
    "                  'reg_lambda': lambda2,'learning_rate': learn_rate}\n",
    "        if obj == 'ce' or obj == 'weightedce':\n",
    "            params['objective'] = 'binary:logistic'\n",
    "        elif obj == 'aec':\n",
    "            params['disable_default_eval_metric'] = True\n",
    "\n",
    "        self.params = params\n",
    "\n",
    "    def fit(self, x_train, y_train, x_val, y_val, cost_matrix_train=None, cost_matrix_val=None):\n",
    "        x_train = np.array(x_train)\n",
    "        y_train = np.array(y_train)\n",
    "        x_val = np.array(x_val)\n",
    "        y_val = np.array(y_val)\n",
    "        cost_matrix_train = np.array(cost_matrix_train)\n",
    "        cost_matrix_val = np.array(cost_matrix_val)\n",
    "        if self.obj == 'ce':\n",
    "            dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "            dval = xgb.DMatrix(x_val, label=y_val)\n",
    "\n",
    "            xgboost = xgb.train(params=self.params, dtrain=dtrain, num_boost_round=500, early_stopping_rounds=50,\n",
    "                            evals=[(dval, 'eval')], verbose_eval=False)\n",
    "\n",
    "        elif self.obj == 'weightedce':\n",
    "            misclass_costs = np.zeros(len(y_train))\n",
    "            misclass_costs[y_train == 0] = cost_matrix_train[:, 1, 0][y_train == 0]\n",
    "            misclass_costs[y_train == 1] = cost_matrix_train[:, 0, 1][y_train == 1]\n",
    "\n",
    "            misclass_costs_val = np.zeros(len(y_val))\n",
    "            misclass_costs_val[y_val == 0] = cost_matrix_val[:, 1, 0][y_val == 0]\n",
    "            misclass_costs_val[y_val == 1] = cost_matrix_val[:, 0, 1][y_val == 1]\n",
    "\n",
    "            dtrain = xgb.DMatrix(x_train, label=y_train, weight=misclass_costs)\n",
    "            dval = xgb.DMatrix(x_val, label=y_val, weight=misclass_costs_val)\n",
    "\n",
    "            xgboost = xgb.train(params=self.params, dtrain=dtrain, num_boost_round=500, early_stopping_rounds=50,\n",
    "                                evals=[(dval, 'eval')], verbose_eval=False)\n",
    "\n",
    "        elif self.obj == 'aec':\n",
    "            dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "            dval = xgb.DMatrix(x_val, label=y_val)\n",
    "\n",
    "            # Do constant computations here to avoid DMatrix error\n",
    "            # diff_costs_train = fixed_cost - y_train * amounts_train\n",
    "\n",
    "            train_constant = (y_train * (cost_matrix_train[:, 1, 1] - cost_matrix_train[:, 0, 1])\n",
    "                              + (1 - y_train) * (cost_matrix_train[:, 1, 0] - cost_matrix_train[:, 0, 0]))\n",
    "\n",
    "            def aec_train(raw_scores, y_true):\n",
    "                scores = expit(raw_scores)\n",
    "\n",
    "                # Average expected cost:\n",
    "                # ec = np.multiply(np.multiply(y_true, (1 - scores)), amounts_train) + np.multiply(scores, fixed_cost)\n",
    "                # ec = y_true * (\n",
    "                #     scores * cost_matrix_train[:, 1, 1] + (1 - scores) * cost_matrix_val[:, 0, 1]) \\\n",
    "                #     + (1 - y_true) * (\n",
    "                #     scores * cost_matrix_val[:, 1, 0] + (1 - scores) * cost_matrix_val[:, 0, 0])\n",
    "\n",
    "                # Gradient\n",
    "                # Use diff_costs_train instead of (fixed_cost - y_true*amounts_train)\n",
    "                # grad = scores * (1 - scores) * diff_costs_train\n",
    "                grad = scores * (1 - scores) * train_constant\n",
    "\n",
    "                # Hessian\n",
    "                hess = np.abs((1 - 2 * scores) * grad)\n",
    "                # hess = scores * (1 - scores) * (1 - 2 * scores) * train_constant\n",
    "\n",
    "                # Grad and hess cannot be too close to 0!\n",
    "                # print(grad.mean())\n",
    "                # print(hess.mean())\n",
    "\n",
    "                return grad, hess\n",
    "\n",
    "            def aec_val(raw_scores, y_true):\n",
    "                scores = expit(raw_scores)\n",
    "\n",
    "                # Return AEC (not grad/hess)\n",
    "                # ec = (1 - scores) * y_val * amounts_val + scores * fixed_cost\n",
    "                # ec = y_true * (\n",
    "                #     scores * cost_matrix_val[:, 1, 1] + (1 - scores) * cost_matrix_val[:, 0, 1]) \\\n",
    "                #     + (1 - y_true) * (\n",
    "                #     scores * cost_matrix_val[:, 1, 0] + (1 - scores) * cost_matrix_val[:, 0, 0])\n",
    "\n",
    "                # Avoid computations with y_true (DMatrix)\n",
    "                if y_true:\n",
    "                    ec = scores * cost_matrix_val[:, 1, 1] + (1 - scores) * cost_matrix_val[:, 0, 1]\n",
    "                else:\n",
    "                    ec = scores * cost_matrix_val[:, 1, 0] + (1 - scores) * cost_matrix_val[:, 0, 0]\n",
    "\n",
    "                aec = ec.mean()\n",
    "\n",
    "                return 'AEC', aec\n",
    "\n",
    "            xgboost = xgb.train(params=self.params, dtrain=dtrain, obj=aec_train, feval=aec_val, num_boost_round=500,\n",
    "                                early_stopping_rounds=50, evals=[(dval, 'eval')], verbose_eval=False)\n",
    "\n",
    "        # print('\\tBest number of trees = %i' % xgboost.best_ntree_limit)\n",
    "\n",
    "        return xgboost\n",
    "\n",
    "    def tune(self, l1, lambda1_list, l2, lambda2_list, learn_rate, learn_ratelist, x_train, y_train, cost_matrix_train, x_val, y_val, cost_matrix_val):\n",
    "        if l1:\n",
    "            self.params['reg_lambda'] = 0\n",
    "            losses_list = []\n",
    "            for lambda1 in lambda1_list:\n",
    "                xgboost = CSBoost(obj=self.obj, lambda1=lambda1)\n",
    "                xgboost = xgboost.fit(x_train, y_train, x_val, y_val, cost_matrix_train, cost_matrix_val)\n",
    "                scores = xgboost.inplace_predict(x_val)\n",
    "\n",
    "                # Evaluate loss (without regularization term!)\n",
    "                if self.obj == 'ce':\n",
    "                    eps = 1e-9  # small value to avoid log(0)\n",
    "                    ce = - (y_val * np.log(scores + eps) + (1 - y_val) * np.log(1 - scores + eps))\n",
    "                    val_loss = ce.mean()\n",
    "                elif self.obj == 'weightedce':\n",
    "                    eps = 1e-9  # small value to avoid log(0)\n",
    "                    ce = - (y_val * np.log(scores + eps) + (1 - y_val) * np.log(1 - scores + eps))\n",
    "\n",
    "                    cost_misclass = np.zeros(len(y_val))\n",
    "                    cost_misclass[y_val == 0] = cost_matrix_val[:, 1, 0][y_val == 0]\n",
    "                    cost_misclass[y_val == 1] = cost_matrix_val[:, 0, 1][y_val == 1]\n",
    "\n",
    "                    weighted_ce = cost_misclass * ce\n",
    "                    val_loss = weighted_ce.mean()\n",
    "                elif self.obj == 'aec':\n",
    "                    def aec_val(raw_scores, y_true):\n",
    "                        scores = expit(raw_scores)\n",
    "\n",
    "                        # Return AEC (not grad/hess)\n",
    "                        # ec = (1 - scores) * y_val * amounts_val + scores * fixed_cost\n",
    "                        ec = y_true * (\n",
    "                            scores * cost_matrix_val[:, 1, 1] + (1 - scores) * cost_matrix_val[:, 0, 1]) \\\n",
    "                            + (1 - y_true) * (\n",
    "                            scores * cost_matrix_val[:, 1, 0] + (1 - scores) * cost_matrix_val[:, 0, 0])\n",
    "\n",
    "                        aec = ec.mean()\n",
    "\n",
    "                        return 'AEC', aec\n",
    "\n",
    "                    aec = aec_val(scores, y_val)\n",
    "                    val_loss = aec[1]\n",
    "                print('\\t\\tLambda l1 = %.5f;\\tLoss = %.5f' % (lambda1, val_loss))\n",
    "                losses_list.append(val_loss)\n",
    "            lambda1_opt = lambda1_list[np.argmin(losses_list)]\n",
    "            print('\\tOptimal lambda = %.5f' % lambda1_opt)\n",
    "            self.params['reg_alpha'] = lambda1_opt\n",
    "        elif l2:\n",
    "            self.params['reg_alpha'] = 0\n",
    "            losses_list = []\n",
    "            for lambda2 in lambda2_list:\n",
    "                xgboost = CSBoost(obj=self.obj, lambda2=lambda2)\n",
    "                xgboost = xgboost.fit(x_train, y_train, x_val, y_val, cost_matrix_train, cost_matrix_val)\n",
    "                scores = xgboost.inplace_predict(x_val)\n",
    "\n",
    "                # Evaluate loss (without regularization term!)\n",
    "                if self.obj == 'ce':\n",
    "                    eps = 1e-9  # small value to avoid log(0)\n",
    "                    ce = - (y_val * np.log(scores + eps) + (1 - y_val) * np.log(1 - scores + eps))\n",
    "                    val_loss = ce.mean()\n",
    "                elif self.obj == 'weightedce':\n",
    "                    eps = 1e-9  # small value to avoid log(0)\n",
    "                    ce = - (y_val * np.log(scores + eps) + (1 - y_val) * np.log(1 - scores + eps))\n",
    "\n",
    "                    cost_misclass = np.zeros(len(y_val))\n",
    "                    cost_misclass[y_val == 0] = cost_matrix_val[:, 1, 0][y_val == 0]\n",
    "                    cost_misclass[y_val == 1] = cost_matrix_val[:, 0, 1][y_val == 1]\n",
    "\n",
    "                    weighted_ce = cost_misclass * ce\n",
    "                    val_loss = weighted_ce.mean()\n",
    "                elif self.obj == 'aec':\n",
    "                    def aec_val(raw_scores, y_true):\n",
    "                        scores = expit(raw_scores)\n",
    "\n",
    "                        # Return AEC (not grad/hess)\n",
    "                        ec = y_true * (\n",
    "                                scores * cost_matrix_val[:, 1, 1] + (1 - scores) * cost_matrix_val[:, 0, 1]) \\\n",
    "                             + (1 - y_true) * (\n",
    "                                     scores * cost_matrix_val[:, 1, 0] + (1 - scores) * cost_matrix_val[:, 0, 0])\n",
    "\n",
    "                        aec = ec.mean()\n",
    "\n",
    "                        return 'AEC', aec\n",
    "\n",
    "                    aec = aec_val(scores, y_val)\n",
    "                    val_loss = aec[1]\n",
    "                print('\\t\\tLambda l2 = %.5f;\\tLoss = %.5f' % (lambda2, val_loss))\n",
    "                losses_list.append(val_loss)\n",
    "            lambda2_opt = lambda2_list[np.argmin(losses_list)]\n",
    "            print('\\tOptimal lambda = %.5f' % lambda2_opt)\n",
    "            self.params['reg_alpha'] = lambda2_opt\n",
    "        elif learn_rate:\n",
    "            self.params['learning_rate'] = 0.01\n",
    "            losses_list = []\n",
    "            for learn_rate in learn_ratelist:\n",
    "                xgboost = CSBoost(obj=self.obj, learn_rate=learn_rate)\n",
    "                xgboost = xgboost.fit(x_train, y_train, x_val, y_val, cost_matrix_train, cost_matrix_val)\n",
    "                scores = xgboost.inplace_predict(x_val)\n",
    "\n",
    "                # Evaluate loss (without regularization term!)\n",
    "                if self.obj == 'ce':\n",
    "                    eps = 1e-9  # small value to avoid log(0)\n",
    "                    ce = - (y_val * np.log(scores + eps) + (1 - y_val) * np.log(1 - scores + eps))\n",
    "                    val_loss = ce.mean()\n",
    "                elif self.obj == 'weightedce':\n",
    "                    eps = 1e-9  # small value to avoid log(0)\n",
    "                    ce = - (y_val * np.log(scores + eps) + (1 - y_val) * np.log(1 - scores + eps))\n",
    "\n",
    "                    cost_misclass = np.zeros(len(y_val))\n",
    "                    cost_misclass[y_val == 0] = cost_matrix_val[:, 1, 0][y_val == 0]\n",
    "                    cost_misclass[y_val == 1] = cost_matrix_val[:, 0, 1][y_val == 1]\n",
    "\n",
    "                    weighted_ce = cost_misclass * ce\n",
    "                    val_loss = weighted_ce.mean()\n",
    "                elif self.obj == 'aec':\n",
    "                    def aec_val(raw_scores, y_true):\n",
    "                        scores = expit(raw_scores)\n",
    "\n",
    "                        # Return AEC (not grad/hess)\n",
    "                        # ec = (1 - scores) * y_val * amounts_val + scores * fixed_cost\n",
    "                        ec = y_true * (\n",
    "                            scores * cost_matrix_val[:, 1, 1] + (1 - scores) * cost_matrix_val[:, 0, 1]) \\\n",
    "                            + (1 - y_true) * (\n",
    "                            scores * cost_matrix_val[:, 1, 0] + (1 - scores) * cost_matrix_val[:, 0, 0])\n",
    "\n",
    "                        aec = ec.mean()\n",
    "\n",
    "                        return 'AEC', aec\n",
    "\n",
    "                    aec = aec_val(scores, y_val)\n",
    "                    val_loss = aec[1]\n",
    "                print('\\t\\tlearning_rate = %.5f;\\tLoss = %.5f' % (learn_rate, val_loss))\n",
    "                losses_list.append(val_loss)\n",
    "            learn_rate_opt = learn_ratelist[np.argmin(losses_list)]\n",
    "            print('\\tOptimal learn_rate = %.5f' % learn_rate_opt)\n",
    "            self.params['learn_rate'] = learn_rate_opt\n",
    "        else:\n",
    "            self.lambda1 = 0\n",
    "            self.lambda2 = 0\n",
    "            self.learn_rate=0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f62e83b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-10T15:55:56.091376Z",
     "start_time": "2022-12-10T15:55:56.061645Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tlearning_rate = 0.01000;\tLoss = 4829.09912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tlearning_rate = 0.02000;\tLoss = 1300.65364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tlearning_rate = 0.05000;\tLoss = 1216.30604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tlearning_rate = 0.10000;\tLoss = 1053.50505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tlearning_rate = 0.15000;\tLoss = 1096.79378\n",
      "\tOptimal learn_rate = 0.10000\n"
     ]
    }
   ],
   "source": [
    "# Grid search for hyperparameters\n",
    "\n",
    "y_train=feature['欺诈状态']\n",
    "x_train=feature.drop(columns=['欺诈状态'])\n",
    "\n",
    "#Split the train and test dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_val, y_train, y_val=train_test_split(x_train, y_train, test_size=0.2, shuffle=True, random_state=333)\n",
    "\n",
    "#Create cost matrix\n",
    "amounts_train = x_train['总审批金额'].values\n",
    "amounts_val = x_val['总审批金额'].values\n",
    "\n",
    "cost_matrix_train = np.zeros((len(x_train), 2, 2))     # cost_matrix [[TN, FN], [FP, TP]]\n",
    "cost_matrix_train[:, 0, 0] = 0.0\n",
    "cost_matrix_train[:, 0, 1] = amounts_train\n",
    "cost_matrix_train[:, 1, 0] = 20000\n",
    "cost_matrix_train[:, 1, 1] = 15000\n",
    "\n",
    "cost_matrix_val = np.zeros((len(x_val), 2, 2))     # cost_matrix [[TN, FN], [FP, TP]]\n",
    "cost_matrix_val[:, 0, 0] = 0.0\n",
    "cost_matrix_val[:, 0, 1] = amounts_val\n",
    "cost_matrix_val[:, 1, 0] = 20000\n",
    "cost_matrix_val[:, 1, 1] = 15000\n",
    "\n",
    "#Train model\n",
    "csboost = CSBoost(obj='aec')\n",
    "csboost.tune(False, [0, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1], False, [0, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1], True,[0.01,0.02,0.05,0.1,0.15], x_train, y_train,cost_matrix_train, x_val, y_val, cost_matrix_val)\n",
    "# lambda1 = 0.01\n",
    "# lambda2 = 0.00001\n",
    "# learn_rate=0.1\n",
    "#csboost=csboost.fit(x_train, y_train, x_val, y_val, cost_matrix_train, cost_matrix_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95d685e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-10T15:56:07.719098Z",
     "start_time": "2022-12-10T15:56:07.705472Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Savings metric\n",
    "def cost_with_algorithm(cost_matrix, labels, predictions):\n",
    "\n",
    "    cost_tn = cost_matrix[:, 0, 0][np.logical_and(predictions == 0, labels == 0)].sum()\n",
    "    cost_fn = cost_matrix[:, 0, 1][np.logical_and(predictions == 0, labels == 1)].sum()\n",
    "    cost_fp = cost_matrix[:, 1, 0][np.logical_and(predictions == 1, labels == 0)].sum()\n",
    "    cost_tp = cost_matrix[:, 1, 1][np.logical_and(predictions == 1, labels == 1)].sum()\n",
    "\n",
    "    return sum((cost_tn, cost_fn, cost_fp, cost_tp))\n",
    "\n",
    "def cost_without_algorithm(cost_matrix, labels):\n",
    "\n",
    "    # Predict everything as the default class that leads to minimal cost\n",
    "    # Also include cost of TP/TN!\n",
    "    cost_neg = cost_matrix[:, 0, 0][labels == 0].sum() + cost_matrix[:, 0, 1][labels == 1].sum()\n",
    "    cost_pos = cost_matrix[:, 1, 0][labels == 0].sum() + cost_matrix[:, 1, 1][labels == 1].sum()\n",
    "\n",
    "    return min(cost_neg, cost_pos)\n",
    "\n",
    "def savings(cost_matrix, labels, predictions):\n",
    "    \n",
    "    cost_without = cost_without_algorithm(cost_matrix, labels)\n",
    "    cost_with = cost_with_algorithm(cost_matrix, labels, predictions)\n",
    "    savings = 1 - cost_with / cost_without\n",
    "    \n",
    "    return savings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5594332c-6b99-4e5f-bdd5-b2e2c01d5028",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_29632\\604130138.py:61: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results_df = pd.concat([results_df, pd.DataFrame([fold_results])], ignore_index=True)\n",
      "D:\\anaconda\\Lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n",
      "D:\\anaconda\\Lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n",
      "D:\\anaconda\\Lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n",
      "D:\\anaconda\\Lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Prepare the data\n",
    "train = feature\n",
    "y = train['欺诈状态']\n",
    "X = train.drop(columns=['欺诈状态'])\n",
    "\n",
    "results_df = pd.DataFrame(columns=['Fold', 'Recall', 'F1-score', 'Precision', 'Savings'])\n",
    "\n",
    "# 5-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=1000)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "    \n",
    "    # Split the train and test dataset\n",
    "    x_train, x_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "    # Creat cost matrix\n",
    "    amounts_train = x_train['总审批金额'].values\n",
    "    amounts_val = x_val['总审批金额'].values\n",
    "    \n",
    "    cost_matrix_train = np.zeros((len(x_train), 2, 2))\n",
    "    cost_matrix_train[:, 0, 0] = 0\n",
    "    cost_matrix_train[:, 0, 1] = amounts_train\n",
    "    cost_matrix_train[:, 1, 0] = 20000\n",
    "    cost_matrix_train[:, 1, 1] = 15000\n",
    "    \n",
    "    cost_matrix_val = np.zeros((len(x_val), 2, 2))\n",
    "    cost_matrix_val[:, 0, 0] = 0\n",
    "    cost_matrix_val[:, 0, 1] = amounts_val\n",
    "    cost_matrix_val[:, 1, 0] = 20000\n",
    "    cost_matrix_val[:, 1, 1] = 15000\n",
    "    \n",
    "    # Train model\n",
    "    csboost = CSBoost(obj='aec')\n",
    "    lambda1 = 0.1\n",
    "    lambda2 = 0.001\n",
    "    learn_rate=0.15\n",
    "    csboost = csboost.fit(x_train, y_train, x_val, y_val, cost_matrix_train, cost_matrix_val)\n",
    "    \n",
    "    # Obtain the predictions\n",
    "    pre_val = expit(csboost.inplace_predict(x_val))\n",
    "    threshold_instance = (cost_matrix_val[:, 1, 0] - cost_matrix_val[:, 0, 0]) / (\n",
    "        cost_matrix_val[:, 1, 0] - cost_matrix_val[:, 0, 0]\n",
    "        + cost_matrix_val[:, 0, 1] - cost_matrix_val[:, 1, 1])\n",
    "    pred = (pre_val > threshold_instance).astype(int)\n",
    "    \n",
    "    # Obtain the performance metrics\n",
    "    fold_results = {\n",
    "        'Fold': fold,\n",
    "        'ACC': round(metrics.accuracy_score(y_val, pred), 4),\n",
    "        'Recall': round(metrics.recall_score(y_val, pred), 4),\n",
    "        'F1-score': round(metrics.f1_score(y_val, pred), 4),\n",
    "        'Precision': round(metrics.precision_score(y_val, pred), 4),\n",
    "        'Savings': round(savings(cost_matrix_val, y_val, pred), 4)\n",
    "    }\n",
    "    \n",
    "    results_df = pd.concat([results_df, pd.DataFrame([fold_results])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "9c6f3376-9380-4b14-bcad-850fa9616fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df.to_csv('results-bnicxgb/bnicxgb10.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5433edf9-d165-4355-951c-22d44cf32a9b",
   "metadata": {},
   "source": [
    "# Basic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d165c197-ae10-43af-b45e-4950907570b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_29632\\638139466.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x_train['检查费发生金额'] = scaler.fit_transform(x_train[['检查费发生金额']])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_29632\\638139466.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x_train['检查费申报金额'] = scaler.fit_transform(x_train[['检查费申报金额']])\n",
      "D:\\anaconda\\Lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n",
      "D:\\anaconda\\Lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tLambda l1 = 0.00000;\tLoss = 4889.13103\n",
      "\t\tLambda l1 = 0.00001;\tLoss = 4889.13103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n",
      "D:\\anaconda\\Lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tLambda l1 = 0.00010;\tLoss = 4889.13104\n",
      "\t\tLambda l1 = 0.00100;\tLoss = 4889.13109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n",
      "D:\\anaconda\\Lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tLambda l1 = 0.01000;\tLoss = 4888.88931\n",
      "\t\tLambda l1 = 0.10000;\tLoss = 4889.24173\n",
      "\tOptimal lambda = 0.01000\n"
     ]
    }
   ],
   "source": [
    "# Grid search for hyperparameters\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Prepare the data\n",
    "amount = pd.read_csv('费用类相关特征2.csv')\n",
    "amount = amount[['个人编码', '检查费发生金额', '检查费申报金额']]\n",
    "train = feature.copy()\n",
    "train = train.reset_index()  \n",
    "train = pd.merge(train, amount, \n",
    "                on='个人编码',  \n",
    "                how='left')     \n",
    "\n",
    "# Fill in missing values\n",
    "train = train.fillna(0)\n",
    "\n",
    "# Prepare the features and label for model\n",
    "x_train = train[['药品费申报金额','药品费发生金额','检查费发生金额','检查费申报金额',\n",
    "                 '治疗费发生金额','治疗费申报金额','基本医疗保险统筹基金支付金额',\n",
    "                 '起付标准以上自负比例金额','总审批金额']]\n",
    "y_train = train['欺诈状态']\n",
    "\n",
    "# Normalization\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "x_train['检查费发生金额'] = scaler.fit_transform(x_train[['检查费发生金额']])\n",
    "x_train['检查费申报金额'] = scaler.fit_transform(x_train[['检查费申报金额']])\n",
    "\n",
    "#Split the train and test dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_val, y_train, y_val=train_test_split(x_train, y_train, test_size=0.2, shuffle=True, random_state=333)\n",
    "\n",
    "#Create cost matrix\n",
    "amounts_train = x_train['总审批金额'].values\n",
    "amounts_val = x_val['总审批金额'].values\n",
    "\n",
    "cost_matrix_train = np.zeros((len(x_train), 2, 2))     # cost_matrix [[TN, FN], [FP, TP]]\n",
    "cost_matrix_train[:, 0, 0] = 0.0\n",
    "cost_matrix_train[:, 0, 1] = amounts_train\n",
    "cost_matrix_train[:, 1, 0] = 20000\n",
    "cost_matrix_train[:, 1, 1] = 15000\n",
    "\n",
    "cost_matrix_val = np.zeros((len(x_val), 2, 2))     # cost_matrix [[TN, FN], [FP, TP]]\n",
    "cost_matrix_val[:, 0, 0] = 0.0\n",
    "cost_matrix_val[:, 0, 1] = amounts_val\n",
    "cost_matrix_val[:, 1, 0] = 20000\n",
    "cost_matrix_val[:, 1, 1] = 15000\n",
    "\n",
    "#Train model\n",
    "csboost = CSBoost(obj='aec')\n",
    "csboost.tune(True, [0, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1], False, [0, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1], False,[0.01,0.02,0.05,0.1,0.15], x_train, y_train,cost_matrix_train, x_val, y_val, cost_matrix_val)\n",
    "# lambda1 = 0.1\n",
    "# lambda2 = 0.1\n",
    "# learn_rate=0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e906a12-5cc3-4931-8bf0-9829e2b8c2fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_29632\\876652144.py:55: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results_df = pd.concat([results_df, pd.DataFrame([fold_results])], ignore_index=True)\n",
      "D:\\anaconda\\Lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n",
      "D:\\anaconda\\Lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n",
      "D:\\anaconda\\Lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n",
      "D:\\anaconda\\Lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Prepare the features and label for model\n",
    "X=train[['药品费申报金额','药品费发生金额','检查费发生金额','检查费申报金额','治疗费发生金额','治疗费申报金额','基本医疗保险统筹基金支付金额','起付标准以上自负比例金额','总审批金额']]\n",
    "y=train['欺诈状态']\n",
    "results_df = pd.DataFrame(columns=['Fold', 'ACC', 'Recall', 'F1-score', 'Precision', 'Savings'])\n",
    "\n",
    "# 5-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=1000)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "    \n",
    "    # Split the train and test dataset\n",
    "    x_train, x_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "    # Creat the cost matrix\n",
    "    amounts_train = x_train['总审批金额'].values\n",
    "    amounts_val = x_val['总审批金额'].values\n",
    "    \n",
    "    cost_matrix_train = np.zeros((len(x_train), 2, 2))\n",
    "    cost_matrix_train[:, 0, 0] = 0\n",
    "    cost_matrix_train[:, 0, 1] = amounts_train\n",
    "    cost_matrix_train[:, 1, 0] = 20000\n",
    "    cost_matrix_train[:, 1, 1] = 15000\n",
    "    \n",
    "    cost_matrix_val = np.zeros((len(x_val), 2, 2))\n",
    "    cost_matrix_val[:, 0, 0] = 0\n",
    "    cost_matrix_val[:, 0, 1] = amounts_val\n",
    "    cost_matrix_val[:, 1, 0] = 20000\n",
    "    cost_matrix_val[:, 1, 1] = 15000\n",
    "    \n",
    "    # Train model\n",
    "    csboost = CSBoost(obj='aec')\n",
    "    lambda1 = 0.1\n",
    "    lambda2 = 0.1\n",
    "    learn_rate=0.15\n",
    "    csboost = csboost.fit(x_train, y_train, x_val, y_val, cost_matrix_train, cost_matrix_val)\n",
    "    \n",
    "    # Obain the predictions\n",
    "    pre_val = expit(csboost.inplace_predict(x_val))\n",
    "    threshold_instance = (cost_matrix_val[:, 1, 0] - cost_matrix_val[:, 0, 0]) / (\n",
    "        cost_matrix_val[:, 1, 0] - cost_matrix_val[:, 0, 0]\n",
    "        + cost_matrix_val[:, 0, 1] - cost_matrix_val[:, 1, 1])\n",
    "    pred = (pre_val > threshold_instance).astype(int)\n",
    "    \n",
    "    # Obtain the performance metrics\n",
    "    fold_results = {\n",
    "        'Fold': fold,\n",
    "        'ACC': round(metrics.accuracy_score(y_val, pred), 4),\n",
    "        'Recall': round(metrics.recall_score(y_val, pred), 4),\n",
    "        'F1-score': round(metrics.f1_score(y_val, pred), 4),\n",
    "        'Precision': round(metrics.precision_score(y_val, pred), 4),\n",
    "        'Savings': round(savings(cost_matrix_val, y_val, pred),4)\n",
    "    }\n",
    "    \n",
    "    results_df = pd.concat([results_df, pd.DataFrame([fold_results])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2bc36aa3-11cf-4396-829d-6817979241df",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('results-basicfeatures/basicfeatures10.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727ca5f5",
   "metadata": {},
   "source": [
    "# BN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e32342cc-5e79-42ad-9502-f276d3a52059",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_29632\\1654312376.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_X = bayes[['主要发生金额','就诊总费用','总审批金额','主要申报金额','滑窗60-3','基本医疗保险统筹基金支付金额极值60','起付标准以上自负比例金额极差60']] = np.int8(X_Kbins)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45c7f5f18e49454aa587514aa9e67441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pgmpy:BayesianModel has been renamed to BayesianNetwork. Please use BayesianNetwork class, BayesianModel will be removed in future.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd9c39126d23487291af6cf9c6176892",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be41b14027d14e36a9a0ed11a46ce9b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pgmpy:BayesianModel has been renamed to BayesianNetwork. Please use BayesianNetwork class, BayesianModel will be removed in future.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e49008e7ccec44de9e5df7a6e88ce8d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8edfff2d161949d081a02e3c7c7a6492",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pgmpy:BayesianModel has been renamed to BayesianNetwork. Please use BayesianNetwork class, BayesianModel will be removed in future.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98e915bc472f4d81979762c2b52a7f7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e10da34b3a714115939c74e061b8918a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pgmpy:BayesianModel has been renamed to BayesianNetwork. Please use BayesianNetwork class, BayesianModel will be removed in future.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c8428dd0adf47b6b324eb72a1cd8dde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c29bfc2d8546416db7cf398323857e74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pgmpy:BayesianModel has been renamed to BayesianNetwork. Please use BayesianNetwork class, BayesianModel will be removed in future.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4669cfe02d18484baf2161acf2db9860",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pgmpy.estimators import BicScore, ExhaustiveSearch, HillClimbSearch\n",
    "from pgmpy.models import BayesianModel\n",
    "from pgmpy.estimators import BayesianEstimator\n",
    "\n",
    "train=feature\n",
    "bayes=train[['主要发生金额','就诊总费用','总审批金额','主要申报金额','滑窗60-3','基本医疗保险统筹基金支付金额极值60','起付标准以上自负比例金额极差60','欺诈状态']]\n",
    "\n",
    "# Discretization of continuous data\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "temp_X = bayes[['主要发生金额','就诊总费用','总审批金额','主要申报金额','滑窗60-3','基本医疗保险统筹基金支付金额极值60','起付标准以上自负比例金额极差60']].values\n",
    "Kbins = KBinsDiscretizer(n_bins = 2, encode = 'ordinal', strategy = 'kmeans')\n",
    "X_Kbins = Kbins.fit_transform(temp_X)\n",
    "temp_X = bayes[['主要发生金额','就诊总费用','总审批金额','主要申报金额','滑窗60-3','基本医疗保险统筹基金支付金额极值60','起付标准以上自负比例金额极差60']] = np.int8(X_Kbins)\n",
    "\n",
    "results_df = pd.DataFrame(columns=['Fold', 'ACC', 'Recall', 'F1-score', 'Precision', 'Savings'])\n",
    "\n",
    "# 5-fold cross-validation\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(bayes)):\n",
    "    \n",
    "    # Split the train and test data\n",
    "    model_train = bayes.iloc[train_index]\n",
    "    test = bayes.iloc[val_index]\n",
    "    model_test = test.drop(['欺诈状态'], axis=1)\n",
    "    \n",
    "    # Construct and train model\n",
    "    hc = HillClimbSearch(model_train)\n",
    "    best_model = hc.estimate()\n",
    "    best_model = BayesianModel(best_model.edges())\n",
    "    best_model.fit(data=model_train, estimator=BayesianEstimator)\n",
    "    \n",
    "    # Obtain the predictions\n",
    "    best_model_pre = best_model.predict(model_test, n_jobs=5)\n",
    "    scores = best_model_pre\n",
    "    y_pred = (scores >= 0.5)*1\n",
    "    \n",
    "    # Calculate the Savings metric\n",
    "    fee = pd.DataFrame()\n",
    "    fee['数额'] = train['总审批金额'].iloc[val_index]\n",
    "    fee.index = test.index  # 确保索引匹配\n",
    "    fee = pd.concat([test, fee], axis=1)\n",
    "    \n",
    "    # Create cost marix\n",
    "    cost_matrix_val = np.zeros((len(x_val), 2, 2))\n",
    "    cost_matrix_val[:, 0, 0] = 0\n",
    "    cost_matrix_val[:, 0, 1] = amounts_val\n",
    "    cost_matrix_val[:, 1, 0] = 20000\n",
    "    cost_matrix_val[:, 1, 1] = 15000\n",
    "    \n",
    "    y_val = y_val.values.flatten()  \n",
    "    y_val = pd.Series(y_val, name='label')\n",
    "    y_val = pd.DataFrame(y_val).reset_index(drop=True)\n",
    "    best_model_pre = pd.DataFrame(best_model_pre, columns=['欺诈状态']).reset_index(drop=True)\n",
    "    yz = pd.concat([y_val, best_model_pre], axis=1)\n",
    "    \n",
    "    saving = savings(cost_matrix_val, yz['label'], yz['欺诈状态'])\n",
    "    \n",
    "    # Obtain the performance metricx\n",
    "    fold_results = {\n",
    "        'Fold': fold,\n",
    "        'ACC': round(metrics.accuracy_score(yz['label'], yz['欺诈状态']), 4),\n",
    "        'Recall': round(metrics.recall_score(yz['label'], yz['欺诈状态']), 4),\n",
    "        'F1-score': round(metrics.f1_score(yz['label'], yz['欺诈状态']), 4),\n",
    "        'Precision': round(metrics.precision_score(yz['label'], yz['欺诈状态']), 4),\n",
    "        'Savings': round(saving, 4)\n",
    "    }\n",
    "    \n",
    "    results_df.loc[len(results_df)] = fold_results\n",
    "\n",
    "#results_df.to_csv('results-bn/bn10.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2725802",
   "metadata": {},
   "source": [
    "# ICXGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a4ce708",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T15:29:57.521905Z",
     "start_time": "2022-12-07T15:29:30.285779Z"
    },
    "code_folding": [
     63
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tlearning_rate = 0.01000;\tLoss = 4840.25772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tlearning_rate = 0.02000;\tLoss = 1248.38718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tlearning_rate = 0.05000;\tLoss = 1281.29254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tlearning_rate = 0.10000;\tLoss = 1055.97155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tlearning_rate = 0.15000;\tLoss = 1103.56474\n",
      "\tOptimal learn_rate = 0.10000\n"
     ]
    }
   ],
   "source": [
    "# Grid search for hyperparameters\n",
    "\n",
    "# Prepare the train and test dataset\n",
    "train = feature.iloc[:, :150]\n",
    "y_train = train['欺诈状态']\n",
    "x_train = train.drop(columns=['欺诈状态'])\n",
    "x_train, x_val, y_train, y_val=train_test_split(x_train, y_train, test_size=0.2, shuffle=True, random_state=333)\n",
    "amounts_train = x_train['总审批金额'].values\n",
    "amounts_val = x_val['总审批金额'].values\n",
    "\n",
    "# Create cost matrix\n",
    "cost_matrix_train = np.zeros((len(x_train), 2, 2))     # cost_matrix [[TN, FN], [FP, TP]]\n",
    "cost_matrix_train[:, 0, 0] = 0\n",
    "cost_matrix_train[:, 0, 1] = amounts_train\n",
    "cost_matrix_train[:, 1, 0] = 20000\n",
    "cost_matrix_train[:, 1, 1] = 15000\n",
    "cost_matrix_val = np.zeros((len(x_val), 2, 2))     # cost_matrix [[TN, FN], [FP, TP]]\n",
    "cost_matrix_val[:, 0, 0] = 0\n",
    "cost_matrix_val[:, 0, 1] = amounts_val\n",
    "cost_matrix_val[:, 1, 0] = 20000\n",
    "cost_matrix_val[:, 1, 1] = 15000\n",
    "\n",
    "# Train model          \n",
    "csboost = CSBoost(obj='aec')\n",
    "csboost.tune(False, [0, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1], False, [0, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1], True, [0.01,0.02,0.05,0.1,0.15], x_train, y_train,cost_matrix_train, x_val, y_val, cost_matrix_val)\n",
    "# lambda1 =0.001\n",
    "# lambda2 =0.0001\n",
    "# learn_rate=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d84bb8b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T15:29:57.521905Z",
     "start_time": "2022-12-07T15:29:30.285779Z"
    },
    "code_folding": [
     63
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_29632\\2087295910.py:54: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results_df = pd.concat([results_df, pd.DataFrame([fold_results])], ignore_index=True)\n",
      "D:\\anaconda\\Lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n",
      "D:\\anaconda\\Lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n",
      "D:\\anaconda\\Lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n",
      "D:\\anaconda\\Lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Prepare the data\n",
    "X=feature.iloc[:, :150]\n",
    "y=feature['欺诈状态']\n",
    "\n",
    "results_df = pd.DataFrame(columns=['Fold', 'Recall', 'F1-score', 'Precision', 'Savings'])\n",
    "\n",
    "# 5-fold cross-validation\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "    \n",
    "    # Split the train and test dataset\n",
    "    x_train, x_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "    # Construct the cost matrix\n",
    "    amounts_train = x_train['总审批金额'].values\n",
    "    amounts_val = x_val['总审批金额'].values\n",
    "    \n",
    "    cost_matrix_train = np.zeros((len(x_train), 2, 2))\n",
    "    cost_matrix_train[:, 0, 0] = 0\n",
    "    cost_matrix_train[:, 0, 1] = amounts_train\n",
    "    cost_matrix_train[:, 1, 0] = 20000\n",
    "    cost_matrix_train[:, 1, 1] = 15000\n",
    "    \n",
    "    cost_matrix_val = np.zeros((len(x_val), 2, 2))\n",
    "    cost_matrix_val[:, 0, 0] = 0\n",
    "    cost_matrix_val[:, 0, 1] = amounts_val\n",
    "    cost_matrix_val[:, 1, 0] = 20000\n",
    "    cost_matrix_val[:, 1, 1] = 15000\n",
    "    \n",
    "    # Train model\n",
    "    csboost = CSBoost(obj='aec')\n",
    "    lambda1 =0.001\n",
    "    lambda2 =0.0001\n",
    "    learn_rate=0.15\n",
    "    csboost = csboost.fit(x_train, y_train, x_val, y_val, cost_matrix_train, cost_matrix_val)\n",
    "    \n",
    "    # Obtain the predictions\n",
    "    pre_val = expit(csboost.inplace_predict(x_val))\n",
    "    threshold_instance = (cost_matrix_val[:, 1, 0] - cost_matrix_val[:, 0, 0]) / (\n",
    "        cost_matrix_val[:, 1, 0] - cost_matrix_val[:, 0, 0]\n",
    "        + cost_matrix_val[:, 0, 1] - cost_matrix_val[:, 1, 1])\n",
    "    pred = (pre_val > threshold_instance).astype(int)\n",
    "    \n",
    "    # Obtain the performance metrics\n",
    "    fold_results = {\n",
    "        'Fold': fold,\n",
    "        'ACC': round(metrics.accuracy_score(y_val, pred), 4),\n",
    "        'Recall': round(metrics.recall_score(y_val, pred), 4),\n",
    "        'F1-score': round(metrics.f1_score(y_val, pred), 4),\n",
    "        'Precision': round(metrics.precision_score(y_val, pred), 4),\n",
    "        'Savings': round(savings(cost_matrix_val, y_val, pred), 4)\n",
    "    }\n",
    "    \n",
    "    results_df = pd.concat([results_df, pd.DataFrame([fold_results])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15a1ade4-dc1e-454f-86a7-ed6c00829429",
   "metadata": {},
   "outputs": [],
   "source": [
    "#results_df.to_csv('results-icxgb/icxgb10.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b37eb8",
   "metadata": {},
   "source": [
    "# LR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0bd4eeb0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T15:29:59.069766Z",
     "start_time": "2022-12-07T15:29:58.873292Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "results_df = pd.DataFrame(columns=['Fold', 'ACC', 'Recall', 'F1-score', 'Precision', 'Savings'])\n",
    "# Prepare the data\n",
    "train = feature\n",
    "y = train['欺诈状态']\n",
    "X = train.drop(columns=['欺诈状态'])\n",
    "\n",
    "# 5-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=333)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "    \n",
    "    # Split the train and test dataset\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "    # Train model\n",
    "    reg = LinearRegression()\n",
    "    reg.fit(X_train, y_train)\n",
    "    \n",
    "    # Obtain the predictions\n",
    "    pred = reg.predict(X_val)\n",
    "    threshold_class_imbalance = 0.5\n",
    "    pred = (pred > threshold_class_imbalance).astype(int)\n",
    "    \n",
    "    # Create the cost matrix\n",
    "    fee = pd.DataFrame()\n",
    "    fee['数额'] = train['总审批金额'].iloc[val_idx]\n",
    "    amounts_val = fee['数额'].values\n",
    "    \n",
    "    cost_matrix_val = np.zeros((len(X_val), 2, 2))\n",
    "    cost_matrix_val[:, 0, 0] = 0\n",
    "    cost_matrix_val[:, 0, 1] = amounts_val\n",
    "    cost_matrix_val[:, 1, 0] = 20000\n",
    "    cost_matrix_val[:, 1, 1] = 15000\n",
    "    \n",
    "    # Obtain the performance metrics\n",
    "    fold_results = {\n",
    "        'Fold': fold,\n",
    "        'ACC': round(metrics.accuracy_score(y_val, pred), 4),\n",
    "        'Recall': round(metrics.recall_score(y_val, pred), 4),\n",
    "        'F1-score': round(metrics.f1_score(y_val, pred), 4),\n",
    "        'Precision': round(metrics.precision_score(y_val, pred), 4),\n",
    "        'Savings': round(savings(cost_matrix_val, y_val, pred), 4)\n",
    "    }\n",
    "    \n",
    "    results_df.loc[len(results_df)] = fold_results\n",
    "\n",
    "#results_df.to_csv('results-lr/lr1.csv', index=False, float_format='%.4f')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1c7a11",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1eb70877",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T15:30:34.736336Z",
     "start_time": "2022-12-07T15:29:59.076750Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "results_df = pd.DataFrame(columns=['Fold', 'ACC', 'Recall', 'F1-score', 'Precision', 'Savings'])\n",
    "\n",
    "# Prepare data\n",
    "train = feature\n",
    "y = train['欺诈状态']\n",
    "X = train.drop(columns=['欺诈状态'])\n",
    "\n",
    "# 5-fold cross-validation\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):    \n",
    "    \n",
    "    # Split the train and test dataset\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]    \n",
    "   \n",
    "    # Train model\n",
    "    clf = SVC(kernel='rbf')\n",
    "    clf.fit(X_train, y_train)    \n",
    "    # Obtain the predictions\n",
    "    pred = clf.predict(X_val)    \n",
    "   \n",
    "    # Create the cost matrix\n",
    "    fee = pd.DataFrame()\n",
    "    fee['数额'] = train['总审批金额'].iloc[val_idx]\n",
    "    amounts_val = fee['数额'].values \n",
    "    cost_matrix_val = np.zeros((len(X_val), 2, 2))\n",
    "    cost_matrix_val[:, 0, 0] = 0\n",
    "    cost_matrix_val[:, 0, 1] = amounts_val\n",
    "    cost_matrix_val[:, 1, 0] = 20000\n",
    "    cost_matrix_val[:, 1, 1] = 15000\n",
    "    \n",
    "    # Obtain the performance metrics\n",
    "    fold_results = {\n",
    "        'Fold': fold,\n",
    "        'ACC': round(metrics.accuracy_score(y_val, pred), 4),\n",
    "        'Recall': round(metrics.recall_score(y_val, pred), 4),\n",
    "        'F1-score': round(metrics.f1_score(y_val, pred), 4),\n",
    "        'Precision': round(metrics.precision_score(y_val, pred), 4),\n",
    "        'Savings': round(savings(cost_matrix_val, y_val, pred), 4)\n",
    "    }\n",
    "    results_df.loc[len(results_df)] = fold_results    \n",
    "# Save results\n",
    "#results_df.to_csv('results-svm/svm5.csv', index=False, float_format='%.4f')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9790d1c",
   "metadata": {},
   "source": [
    "# XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1db1d3e9-0110-4f0e-895e-03a4ea0a2829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Best: 0.200783 using {'reg_lambda': 0.001}\n"
     ]
    }
   ],
   "source": [
    "# Grid search for hyperparameters\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "train=feature.iloc[:, :150]\n",
    "y_train=train['欺诈状态']\n",
    "x_train=train.drop(columns=['欺诈状态'])\n",
    "x_train,x_val,y_train,y_val=train_test_split(x_train, y_train,test_size=0.2,shuffle=True,random_state=333)\n",
    "cv_params = {'reg_lambda': [0, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1]}\n",
    "other_params = {'min_child_weight': 1, 'seed': 0, 'n_estimators':500,'learning_rate':0.1,'max_depth': 20,'reg_alpha':0,\n",
    "                    'subsample': 0.8, 'colsample_bytree': 0.8, 'gamma': 0}\n",
    "tree_model = XGBClassifier(**other_params)\n",
    "optimized_GBM = GridSearchCV(estimator=tree_model,param_grid=cv_params,scoring='r2',cv=5,verbose=1, n_jobs=4)\n",
    "grid_result = optimized_GBM.fit(x_train, y_train)\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "80ddcb50-15a6-444a-beb9-35f770b5e851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyper-parameters\n",
    "params = {\n",
    "    'min_child_weight': 1, \n",
    "    'seed': 0, \n",
    "    'n_estimators': 500,\n",
    "    'max_depth': 20,\n",
    "    'learning_rate': 0.1,\n",
    "    'subsample': 0.8, \n",
    "    'colsample_bytree': 0.8, \n",
    "    'gamma': 0, \n",
    "    'reg_alpha': 0, \n",
    "    'reg_lambda': 1\n",
    "}\n",
    "results_df = pd.DataFrame(columns=['Fold', 'ACC', 'Recall', 'F1-score', 'Precision', 'Savings'])\n",
    "\n",
    "# Prepare data\n",
    "train=feature.iloc[:, :150]\n",
    "y_train=train['欺诈状态']\n",
    "x_train=train.drop(columns=['欺诈状态'])\n",
    "\n",
    "# 5-fold cross-validation\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(X)):\n",
    "    \n",
    "    # Split the train and test dataset\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "    \n",
    "    # Train model\n",
    "    tree_model = XGBClassifier(**params)\n",
    "    tree_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Obtain the predictions\n",
    "    pred = tree_model.predict(X_val)\n",
    "    \n",
    "    # Create the cost matrix\n",
    "    fee = pd.DataFrame()\n",
    "    fee['数额'] = train['总审批金额'].iloc[val_idx]\n",
    "    amounts_val = fee['数额'].values\n",
    "    cost_matrix_val = np.zeros((len(X_val), 2, 2))\n",
    "    cost_matrix_val[:, 0, 0] = 0\n",
    "    cost_matrix_val[:, 0, 1] = amounts_val\n",
    "    cost_matrix_val[:, 1, 0] = 20000\n",
    "    cost_matrix_val[:, 1, 1] = 15000   \n",
    "    \n",
    "    # Obtain the performance metrics\n",
    "    fold_results = {\n",
    "        'Fold': fold,\n",
    "        'ACC': round(metrics.accuracy_score(y_val, pred), 4),\n",
    "        'Recall': round(metrics.recall_score(y_val, pred), 4),\n",
    "        'F1-score': round(metrics.f1_score(y_val, pred), 4),\n",
    "        'Precision': round(metrics.precision_score(y_val, pred), 4),\n",
    "        'Savings': round(savings(cost_matrix_val, y_val, pred), 4)\n",
    "    }\n",
    "    results_df.loc[len(results_df)] = fold_results    \n",
    "# Save results\n",
    "#results_df.to_csv('results-xgb/xgb10.csv', index=False, float_format='%.4f')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701aa6cf",
   "metadata": {},
   "source": [
    "# DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b910e19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T15:31:37.541298Z",
     "start_time": "2022-12-07T15:31:31.343883Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3 Complete [00h 00m 05s]\n",
      "val_accuracy: 0.9512500166893005\n",
      "\n",
      "Best val_accuracy So Far: 0.9514999985694885\n",
      "Total elapsed time: 00h 00m 17s\n",
      "\n",
      "The optimal number of units in the first densely-connected layer is 64.\n",
      "The optimal dropout rate is 0.2.\n",
      "The optimal learning rate for the optimizer is 0.001.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "\n",
    "from sklearn import metrics\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import keras_tuner as kt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Step 2: Prepare your data\n",
    "train=feature.iloc[:, :150]\n",
    "y_train=train['欺诈状态']\n",
    "x_train=train.drop(columns=['欺诈状态'])\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, shuffle=True, random_state=333)\n",
    "\n",
    "# Step 3: Define a model-building function for Keras Tuner\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Hyperparameter for the number of neurons in the first hidden layer\n",
    "    hp_units = hp.Int('units', min_value=32, max_value=64, step=32)\n",
    "    model.add(Dense(units=hp_units, activation='relu', input_dim=x_train.shape[1]))\n",
    "    \n",
    "    # Hyperparameter for dropout rate\n",
    "    hp_dropout = hp.Float('dropout', min_value=0.0, max_value=0.2, step=0.1)\n",
    "    model.add(Dropout(hp_dropout))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Hyperparameter for learning rate\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "    model.compile(optimizer=Adam(learning_rate=hp_learning_rate),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Step 4: Initialize Keras Tuner\n",
    "\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=3,\n",
    "    executions_per_trial=1,\n",
    "    directory='my_dir',\n",
    "    project_name='intro_to_kt',\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "# Step 5: Run the hyperparameter search\n",
    "tuner.search(x_train, y_train, epochs=10, validation_data=(x_val, y_val))\n",
    "\n",
    "# Step 6: Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The optimal number of units in the first densely-connected layer is {best_hps.get('units')}.\n",
    "The optimal dropout rate is {best_hps.get('dropout')}.\n",
    "The optimal learning rate for the optimizer is {best_hps.get('learning_rate')}.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0f4271b-5591-4129-a233-070c307355b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 790us/step - accuracy: 0.8524 - loss: 130.4891 - val_accuracy: 0.9542 - val_loss: 23.4734\n",
      "Epoch 2/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 668us/step - accuracy: 0.8475 - loss: 21.8694 - val_accuracy: 0.9542 - val_loss: 0.4288\n",
      "Epoch 3/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - accuracy: 0.9104 - loss: 0.5390 - val_accuracy: 0.9542 - val_loss: 0.3141\n",
      "Epoch 4/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - accuracy: 0.9207 - loss: 0.4803 - val_accuracy: 0.9542 - val_loss: 0.2626\n",
      "Epoch 5/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - accuracy: 0.9367 - loss: 0.3895 - val_accuracy: 0.9542 - val_loss: 0.2767\n",
      "Epoch 6/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 671us/step - accuracy: 0.9441 - loss: 0.3531 - val_accuracy: 0.9542 - val_loss: 0.2350\n",
      "Epoch 7/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - accuracy: 0.9454 - loss: 0.2814 - val_accuracy: 0.9542 - val_loss: 0.2226\n",
      "Epoch 8/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - accuracy: 0.9496 - loss: 0.2375 - val_accuracy: 0.9553 - val_loss: 0.2263\n",
      "Epoch 9/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 643us/step - accuracy: 0.9490 - loss: 0.2193 - val_accuracy: 0.9542 - val_loss: 0.1955\n",
      "Epoch 10/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 641us/step - accuracy: 0.9490 - loss: 0.2037 - val_accuracy: 0.9535 - val_loss: 0.1880\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 351us/step\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 811us/step - accuracy: 0.8356 - loss: 181.7928 - val_accuracy: 0.9465 - val_loss: 42.5467\n",
      "Epoch 2/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step - accuracy: 0.8627 - loss: 43.3309 - val_accuracy: 0.9465 - val_loss: 8.7074\n",
      "Epoch 3/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - accuracy: 0.8651 - loss: 3.5272 - val_accuracy: 0.9465 - val_loss: 0.3304\n",
      "Epoch 4/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - accuracy: 0.9247 - loss: 0.4782 - val_accuracy: 0.9465 - val_loss: 0.3111\n",
      "Epoch 5/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 670us/step - accuracy: 0.9310 - loss: 0.4229 - val_accuracy: 0.9465 - val_loss: 0.2988\n",
      "Epoch 6/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 671us/step - accuracy: 0.9403 - loss: 0.3874 - val_accuracy: 0.9465 - val_loss: 0.2898\n",
      "Epoch 7/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - accuracy: 0.9457 - loss: 0.3341 - val_accuracy: 0.9465 - val_loss: 0.3338\n",
      "Epoch 8/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - accuracy: 0.9477 - loss: 0.2863 - val_accuracy: 0.9488 - val_loss: 0.2511\n",
      "Epoch 9/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 643us/step - accuracy: 0.9507 - loss: 0.2496 - val_accuracy: 0.9538 - val_loss: 0.2509\n",
      "Epoch 10/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - accuracy: 0.9539 - loss: 0.2172 - val_accuracy: 0.9465 - val_loss: 0.2533\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 355us/step\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 818us/step - accuracy: 0.8594 - loss: 132.2688 - val_accuracy: 0.9525 - val_loss: 21.9329\n",
      "Epoch 2/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 676us/step - accuracy: 0.8490 - loss: 26.4343 - val_accuracy: 0.9525 - val_loss: 1.8919\n",
      "Epoch 3/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 665us/step - accuracy: 0.8876 - loss: 1.3165 - val_accuracy: 0.9525 - val_loss: 0.3938\n",
      "Epoch 4/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 676us/step - accuracy: 0.9269 - loss: 0.4434 - val_accuracy: 0.9525 - val_loss: 0.2987\n",
      "Epoch 5/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - accuracy: 0.9289 - loss: 0.4288 - val_accuracy: 0.9525 - val_loss: 0.5772\n",
      "Epoch 6/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 677us/step - accuracy: 0.9406 - loss: 0.3615 - val_accuracy: 0.9525 - val_loss: 0.2575\n",
      "Epoch 7/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - accuracy: 0.9402 - loss: 0.3223 - val_accuracy: 0.9525 - val_loss: 0.2443\n",
      "Epoch 8/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - accuracy: 0.9500 - loss: 0.2697 - val_accuracy: 0.9572 - val_loss: 0.2462\n",
      "Epoch 9/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 677us/step - accuracy: 0.9486 - loss: 0.2445 - val_accuracy: 0.9525 - val_loss: 0.2274\n",
      "Epoch 10/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 677us/step - accuracy: 0.9505 - loss: 0.2181 - val_accuracy: 0.9528 - val_loss: 0.1957\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 323us/step\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 789us/step - accuracy: 0.6507 - loss: 773.2514 - val_accuracy: 0.9457 - val_loss: 77.7722\n",
      "Epoch 2/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - accuracy: 0.8566 - loss: 96.4103 - val_accuracy: 0.9457 - val_loss: 29.4903\n",
      "Epoch 3/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 674us/step - accuracy: 0.8508 - loss: 33.0281 - val_accuracy: 0.9457 - val_loss: 3.4975\n",
      "Epoch 4/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - accuracy: 0.8807 - loss: 2.6137 - val_accuracy: 0.9457 - val_loss: 0.6384\n",
      "Epoch 5/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 674us/step - accuracy: 0.9270 - loss: 0.4459 - val_accuracy: 0.9457 - val_loss: 0.3909\n",
      "Epoch 6/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - accuracy: 0.9253 - loss: 0.4648 - val_accuracy: 0.9457 - val_loss: 0.4085\n",
      "Epoch 7/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 679us/step - accuracy: 0.9371 - loss: 0.3980 - val_accuracy: 0.9457 - val_loss: 0.2922\n",
      "Epoch 8/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - accuracy: 0.9309 - loss: 0.3907 - val_accuracy: 0.9457 - val_loss: 0.2842\n",
      "Epoch 9/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - accuracy: 0.9351 - loss: 0.3603 - val_accuracy: 0.9457 - val_loss: 0.2402\n",
      "Epoch 10/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - accuracy: 0.9406 - loss: 0.3231 - val_accuracy: 0.9457 - val_loss: 0.2338\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 363us/step\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 819us/step - accuracy: 0.8682 - loss: 103.0213 - val_accuracy: 0.9510 - val_loss: 21.9135\n",
      "Epoch 2/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - accuracy: 0.8574 - loss: 20.5963 - val_accuracy: 0.9510 - val_loss: 0.9471\n",
      "Epoch 3/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 665us/step - accuracy: 0.8936 - loss: 0.8486 - val_accuracy: 0.9510 - val_loss: 0.3356\n",
      "Epoch 4/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - accuracy: 0.9326 - loss: 0.4095 - val_accuracy: 0.9510 - val_loss: 0.4094\n",
      "Epoch 5/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 666us/step - accuracy: 0.9388 - loss: 0.3775 - val_accuracy: 0.9510 - val_loss: 0.2915\n",
      "Epoch 6/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 646us/step - accuracy: 0.9401 - loss: 0.3578 - val_accuracy: 0.9510 - val_loss: 0.2576\n",
      "Epoch 7/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 668us/step - accuracy: 0.9436 - loss: 0.3380 - val_accuracy: 0.9510 - val_loss: 0.2367\n",
      "Epoch 8/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 668us/step - accuracy: 0.9471 - loss: 0.2680 - val_accuracy: 0.9510 - val_loss: 0.2302\n",
      "Epoch 9/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 646us/step - accuracy: 0.9470 - loss: 0.2558 - val_accuracy: 0.9510 - val_loss: 0.2099\n",
      "Epoch 10/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - accuracy: 0.9496 - loss: 0.2255 - val_accuracy: 0.9510 - val_loss: 0.1971\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 347us/step\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(columns=['Fold', 'ACC', 'Recall', 'F1-score', 'Precision', 'Savings'])\n",
    "y = train['欺诈状态']\n",
    "X = train.drop(columns=['欺诈状态'])\n",
    "\n",
    "# 5-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=1000)\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(X)):    \n",
    "    \n",
    "    # Split the train and test dataset\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "    \n",
    "    # Train model\n",
    "    model = tuner.hypermodel.build(best_hps)\n",
    "    history = model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val), verbose=1)\n",
    "    \n",
    "    # Obtain the predictions\n",
    "    pred = (model.predict(X_val) > 0.05).astype(\"int32\").flatten()  # 确保pred是一维数组\n",
    "    y_val = y_val.values  # 转换为numpy数组\n",
    "    \n",
    "    # Create cost matrix\n",
    "    fee = pd.DataFrame()\n",
    "    fee['数额'] = train['总审批金额'].iloc[val_index]\n",
    "    amounts_val = fee['数额'].values    \n",
    "    n_samples = len(amounts_val)\n",
    "    cost_matrix_val = np.zeros((n_samples, 2, 2))\n",
    "    cost_matrix_val[:, 0, 0] = 0\n",
    "    cost_matrix_val[:, 0, 1] = amounts_val\n",
    "    cost_matrix_val[:, 1, 0] = 20000\n",
    "    cost_matrix_val[:, 1, 1] = 15000\n",
    "    \n",
    "    # Savings metric\n",
    "    def cost_with_algorithm1(cost_matrix, labels, predictions):\n",
    "        cost = 0\n",
    "        for i in range(len(labels)):\n",
    "            cost += cost_matrix[i, predictions[i], labels[i]]\n",
    "        return cost\n",
    "\n",
    "    def cost_without_algorithm1(cost_matrix, labels):\n",
    "        cost_all_negative = sum(cost_matrix[i, 0, labels[i]] for i in range(len(labels)))\n",
    "        cost_all_positive = sum(cost_matrix[i, 1, labels[i]] for i in range(len(labels)))\n",
    "        return min(cost_all_negative, cost_all_positive)\n",
    "\n",
    "    def savings1(cost_matrix, labels, predictions):\n",
    "        cost_without = cost_without_algorithm1(cost_matrix, labels)\n",
    "        cost_with = cost_with_algorithm1(cost_matrix, labels, predictions)\n",
    "        savings = 1 - cost_with / cost_without\n",
    "        return savings\n",
    "    \n",
    "    # Obtain the performance metrics\n",
    "    fold_results = {\n",
    "        'Fold': fold,\n",
    "        'ACC': round(metrics.accuracy_score(y_val, pred), 4),\n",
    "        'Recall': round(metrics.recall_score(y_val, pred), 4),\n",
    "        'F1-score': round(metrics.f1_score(y_val, pred), 4),\n",
    "        'Precision': round(metrics.precision_score(y_val, pred), 4),\n",
    "        'Savings': round(savings1(cost_matrix_val, y_val, pred), 4)\n",
    "    }\n",
    "    \n",
    "    results_df.loc[len(results_df)] = fold_results\n",
    "    \n",
    "# Save results\n",
    "#results_df.to_csv('results-dnn/dnn10.csv', index=False, float_format='%.4f')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a10deaa",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "948ce989",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-12-07T15:27:32.826Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3 Complete [00h 00m 06s]\n",
      "val_accuracy: 0.9512500166893005\n",
      "\n",
      "Best val_accuracy So Far: 0.9512500166893005\n",
      "Total elapsed time: 00h 00m 18s\n",
      "\n",
      "The optimal number of units in the first LSTM layer is 64.\n",
      "The optimal dropout rate is 0.1.\n",
      "The optimal learning rate for the optimizer is 0.0001.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import LSTM\n",
    "# Prepare data\n",
    "train = feature.iloc[:, :150]\n",
    "y_train = train['欺诈状态']\n",
    "x_train = train.drop(columns=['欺诈状态'])\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, shuffle=True, random_state=333)\n",
    "\n",
    "# Reshape the data for LSTM (samples, time steps, features)\n",
    "x_train = np.reshape(x_train.values, (x_train.shape[0], 1, x_train.shape[1]))\n",
    "x_val = np.reshape(x_val.values, (x_val.shape[0], 1, x_val.shape[1]))\n",
    "\n",
    "# Define a model-building function for Keras Tuner\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # First LSTM layer\n",
    "    hp_units = hp.Int('units', min_value=32, max_value=64, step=32)\n",
    "    model.add(LSTM(units=hp_units, \n",
    "                  return_sequences=True,\n",
    "                  input_shape=(1, x_train.shape[2])))\n",
    "    \n",
    "    # Dropout after first LSTM\n",
    "    hp_dropout = hp.Float('dropout', min_value=0.0, max_value=0.2, step=0.1)\n",
    "    model.add(Dropout(hp_dropout))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Hyperparameter for learning rate\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "    model.compile(optimizer=Adam(learning_rate=hp_learning_rate),\n",
    "                 loss='binary_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Initialize Keras Tuner\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=3,\n",
    "    executions_per_trial=1,\n",
    "    directory='my_dir',\n",
    "    project_name='lstm_kt',\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "# Run the hyperparameter search\n",
    "tuner.search(x_train, y_train, epochs=10, validation_data=(x_val, y_val))\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The optimal number of units in the first LSTM layer is {best_hps.get('units')}.\n",
    "The optimal dropout rate is {best_hps.get('dropout')}.\n",
    "The optimal learning rate for the optimizer is {best_hps.get('learning_rate')}.\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "91f23b5b-b25e-4860-b9db-9d19423f31a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9247 - loss: 0.4074 - val_accuracy: 0.9542 - val_loss: 0.2159\n",
      "Epoch 2/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 942us/step - accuracy: 0.9505 - loss: 0.2203 - val_accuracy: 0.9542 - val_loss: 0.1861\n",
      "Epoch 3/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 927us/step - accuracy: 0.9526 - loss: 0.1927 - val_accuracy: 0.9542 - val_loss: 0.1859\n",
      "Epoch 4/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 940us/step - accuracy: 0.9502 - loss: 0.1996 - val_accuracy: 0.9542 - val_loss: 0.1859\n",
      "Epoch 5/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 952us/step - accuracy: 0.9454 - loss: 0.2145 - val_accuracy: 0.9542 - val_loss: 0.1859\n",
      "Epoch 6/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 939us/step - accuracy: 0.9481 - loss: 0.2057 - val_accuracy: 0.9542 - val_loss: 0.1859\n",
      "Epoch 7/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 938us/step - accuracy: 0.9496 - loss: 0.2013 - val_accuracy: 0.9542 - val_loss: 0.1857\n",
      "Epoch 8/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 935us/step - accuracy: 0.9479 - loss: 0.2072 - val_accuracy: 0.9542 - val_loss: 0.1858\n",
      "Epoch 9/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 940us/step - accuracy: 0.9490 - loss: 0.2026 - val_accuracy: 0.9542 - val_loss: 0.1874\n",
      "Epoch 10/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 944us/step - accuracy: 0.9473 - loss: 0.2095 - val_accuracy: 0.9542 - val_loss: 0.1858\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 486us/step\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9466 - loss: 0.4360 - val_accuracy: 0.9465 - val_loss: 0.3087\n",
      "Epoch 2/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 940us/step - accuracy: 0.9529 - loss: 0.2831 - val_accuracy: 0.9465 - val_loss: 0.2223\n",
      "Epoch 3/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 964us/step - accuracy: 0.9505 - loss: 0.2115 - val_accuracy: 0.9465 - val_loss: 0.2111\n",
      "Epoch 4/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 924us/step - accuracy: 0.9517 - loss: 0.1990 - val_accuracy: 0.9465 - val_loss: 0.2089\n",
      "Epoch 5/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 935us/step - accuracy: 0.9517 - loss: 0.1962 - val_accuracy: 0.9465 - val_loss: 0.2097\n",
      "Epoch 6/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 938us/step - accuracy: 0.9494 - loss: 0.2030 - val_accuracy: 0.9465 - val_loss: 0.2103\n",
      "Epoch 7/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 938us/step - accuracy: 0.9495 - loss: 0.2028 - val_accuracy: 0.9465 - val_loss: 0.2105\n",
      "Epoch 8/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 947us/step - accuracy: 0.9518 - loss: 0.1959 - val_accuracy: 0.9465 - val_loss: 0.2094\n",
      "Epoch 9/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 937us/step - accuracy: 0.9518 - loss: 0.1959 - val_accuracy: 0.9465 - val_loss: 0.2097\n",
      "Epoch 10/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 936us/step - accuracy: 0.9497 - loss: 0.2016 - val_accuracy: 0.9465 - val_loss: 0.2106\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 476us/step\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "D:\\anaconda\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9291 - loss: 0.4729 - val_accuracy: 0.9525 - val_loss: 0.2336\n",
      "Epoch 2/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 946us/step - accuracy: 0.9479 - loss: 0.2380 - val_accuracy: 0.9525 - val_loss: 0.2005\n",
      "Epoch 3/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 940us/step - accuracy: 0.9515 - loss: 0.2037 - val_accuracy: 0.9525 - val_loss: 0.1915\n",
      "Epoch 4/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 939us/step - accuracy: 0.9513 - loss: 0.1961 - val_accuracy: 0.9525 - val_loss: 0.1914\n",
      "Epoch 5/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 932us/step - accuracy: 0.9494 - loss: 0.2024 - val_accuracy: 0.9525 - val_loss: 0.1912\n",
      "Epoch 6/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 932us/step - accuracy: 0.9515 - loss: 0.1953 - val_accuracy: 0.9525 - val_loss: 0.1912\n",
      "Epoch 7/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 936us/step - accuracy: 0.9506 - loss: 0.1973 - val_accuracy: 0.9525 - val_loss: 0.1912\n",
      "Epoch 8/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 934us/step - accuracy: 0.9490 - loss: 0.2029 - val_accuracy: 0.9525 - val_loss: 0.1911\n",
      "Epoch 9/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 931us/step - accuracy: 0.9500 - loss: 0.1985 - val_accuracy: 0.9525 - val_loss: 0.1911\n",
      "Epoch 10/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 946us/step - accuracy: 0.9475 - loss: 0.2062 - val_accuracy: 0.9525 - val_loss: 0.1911\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 500us/step\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9512 - loss: 0.3187 - val_accuracy: 0.9457 - val_loss: 0.2176\n",
      "Epoch 2/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 972us/step - accuracy: 0.9495 - loss: 0.2075 - val_accuracy: 0.9457 - val_loss: 0.2113\n",
      "Epoch 3/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 936us/step - accuracy: 0.9505 - loss: 0.2007 - val_accuracy: 0.9457 - val_loss: 0.2109\n",
      "Epoch 4/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 936us/step - accuracy: 0.9555 - loss: 0.1845 - val_accuracy: 0.9457 - val_loss: 0.2137\n",
      "Epoch 5/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 930us/step - accuracy: 0.9523 - loss: 0.1943 - val_accuracy: 0.9457 - val_loss: 0.2149\n",
      "Epoch 6/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 933us/step - accuracy: 0.9499 - loss: 0.2022 - val_accuracy: 0.9457 - val_loss: 0.2134\n",
      "Epoch 7/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 941us/step - accuracy: 0.9522 - loss: 0.1988 - val_accuracy: 0.9457 - val_loss: 0.2188\n",
      "Epoch 8/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 930us/step - accuracy: 0.9510 - loss: 0.2009 - val_accuracy: 0.9457 - val_loss: 0.2129\n",
      "Epoch 9/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 937us/step - accuracy: 0.9549 - loss: 0.1862 - val_accuracy: 0.9457 - val_loss: 0.2123\n",
      "Epoch 10/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 958us/step - accuracy: 0.9507 - loss: 0.1981 - val_accuracy: 0.9457 - val_loss: 0.2125\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 471us/step\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9501 - loss: 0.2468 - val_accuracy: 0.9510 - val_loss: 0.1966\n",
      "Epoch 2/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 944us/step - accuracy: 0.9501 - loss: 0.2008 - val_accuracy: 0.9510 - val_loss: 0.1984\n",
      "Epoch 3/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 941us/step - accuracy: 0.9510 - loss: 0.1988 - val_accuracy: 0.9510 - val_loss: 0.1964\n",
      "Epoch 4/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 938us/step - accuracy: 0.9475 - loss: 0.2088 - val_accuracy: 0.9510 - val_loss: 0.1960\n",
      "Epoch 5/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 932us/step - accuracy: 0.9470 - loss: 0.2093 - val_accuracy: 0.9510 - val_loss: 0.1960\n",
      "Epoch 6/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 934us/step - accuracy: 0.9513 - loss: 0.1968 - val_accuracy: 0.9510 - val_loss: 0.1956\n",
      "Epoch 7/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 942us/step - accuracy: 0.9531 - loss: 0.1901 - val_accuracy: 0.9510 - val_loss: 0.1959\n",
      "Epoch 8/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 945us/step - accuracy: 0.9491 - loss: 0.2052 - val_accuracy: 0.9510 - val_loss: 0.1964\n",
      "Epoch 9/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 938us/step - accuracy: 0.9514 - loss: 0.1958 - val_accuracy: 0.9510 - val_loss: 0.1956\n",
      "Epoch 10/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 925us/step - accuracy: 0.9500 - loss: 0.2007 - val_accuracy: 0.9510 - val_loss: 0.1957\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 476us/step\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(columns=['Fold', 'ACC', 'Recall', 'F1-score', 'Precision', 'Savings'])\n",
    "y = train['欺诈状态']\n",
    "X = train.drop(columns=['欺诈状态'])\n",
    "\n",
    "# 5-fold cross-validation\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(X)):\n",
    "    \n",
    "    # Split the train and test dataset\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    # Reshape\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], 1, X_val.shape[1]))\n",
    "\n",
    "    # Train model\n",
    "    model = tuner.hypermodel.build(best_hps)\n",
    "    history = model.fit(X_train, y_train, \n",
    "                       epochs=10, \n",
    "                       validation_data=(X_val, y_val), \n",
    "                       verbose=1)\n",
    "    \n",
    "    # Obtain the predictions\n",
    "    pred = (model.predict(X_val) > 0.05).astype(\"int32\").flatten()  # 确保pred是一维数组\n",
    "    y_val = y_val.values  # 转换为numpy数组\n",
    "    \n",
    "    # Create the cost matrix\n",
    "    fee = pd.DataFrame()\n",
    "    fee['数额'] = train['总审批金额'].iloc[val_index]\n",
    "    amounts_val = fee['数额'].values\n",
    "    \n",
    "    n_samples = len(amounts_val)\n",
    "    cost_matrix_val = np.zeros((n_samples, 2, 2))\n",
    "    cost_matrix_val[:, 0, 0] = 0\n",
    "    cost_matrix_val[:, 0, 1] = amounts_val\n",
    "    cost_matrix_val[:, 1, 0] = 20000\n",
    "    cost_matrix_val[:, 1, 1] = 15000\n",
    "    \n",
    "    # Obtain the performance metrics\n",
    "    fold_results = {\n",
    "        'Fold': fold,\n",
    "        'ACC': round(metrics.accuracy_score(y_val, pred), 4),\n",
    "        'Recall': round(metrics.recall_score(y_val, pred), 4),\n",
    "        'F1-score': round(metrics.f1_score(y_val, pred), 4),\n",
    "        'Precision': round(metrics.precision_score(y_val, pred), 4),\n",
    "        'Savings': round(savings1(cost_matrix_val, y_val, pred), 4)\n",
    "    }\n",
    "    results_df.loc[len(results_df)] = fold_results\n",
    "    \n",
    "# Save results\n",
    "#results_df.to_csv('results-lstm/lstm10.csv', index=False, float_format='%.4f')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e510364",
   "metadata": {},
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a17534d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-12-07T15:27:32.858Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3 Complete [00h 00m 06s]\n",
      "val_accuracy: 0.9512500166893005\n",
      "\n",
      "Best val_accuracy So Far: 0.9512500166893005\n",
      "Total elapsed time: 00h 00m 18s\n",
      "\n",
      "The optimal number of units in the first GRU layer is 32.\n",
      "The optimal dropout rate is 0.0.\n",
      "The optimal learning rate for the optimizer is 0.01.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import GRU\n",
    "\n",
    "# Prepare data\n",
    "train = feature.iloc[:, :150]\n",
    "y_train = train['欺诈状态']\n",
    "x_train = train.drop(columns=['欺诈状态'])\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, shuffle=True, random_state=333)\n",
    "\n",
    "# Reshape the data for GRU (samples, time steps, features)\n",
    "x_train = np.reshape(x_train.values, (x_train.shape[0], 1, x_train.shape[1]))\n",
    "x_val = np.reshape(x_val.values, (x_val.shape[0], 1, x_val.shape[1]))\n",
    "\n",
    "# Define a model-building function for Keras Tuner\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # First GRU layer\n",
    "    hp_units = hp.Int('units', min_value=32, max_value=64, step=32)\n",
    "    model.add(GRU(units=hp_units, \n",
    "                 return_sequences=True,\n",
    "                 input_shape=(1, x_train.shape[2])))\n",
    "    \n",
    "    # Dropout after first GRU\n",
    "    hp_dropout = hp.Float('dropout', min_value=0.0, max_value=0.3, step=0.1)\n",
    "    model.add(Dropout(hp_dropout))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Hyperparameter for learning rate\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "    model.compile(optimizer=Adam(learning_rate=hp_learning_rate),\n",
    "                 loss='binary_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Initialize Keras Tuner\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=3,\n",
    "    executions_per_trial=1,\n",
    "    directory='my_dir',\n",
    "    project_name='gru_kt',\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "# Run the hyperparameter search\n",
    "tuner.search(x_train, y_train, epochs=10, validation_data=(x_val, y_val))\n",
    " \n",
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The optimal number of units in the first GRU layer is {best_hps.get('units')}.\n",
    "The optimal dropout rate is {best_hps.get('dropout')}.\n",
    "The optimal learning rate for the optimizer is {best_hps.get('learning_rate')}.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9088dd02-7445-4c35-8bcb-47104f1acf08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9343 - loss: 0.2318 - val_accuracy: 0.9542 - val_loss: 0.1885\n",
      "Epoch 2/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 908us/step - accuracy: 0.9486 - loss: 0.2046 - val_accuracy: 0.9542 - val_loss: 0.1870\n",
      "Epoch 3/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 895us/step - accuracy: 0.9481 - loss: 0.2057 - val_accuracy: 0.9542 - val_loss: 0.1874\n",
      "Epoch 4/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 894us/step - accuracy: 0.9518 - loss: 0.1952 - val_accuracy: 0.9542 - val_loss: 0.1879\n",
      "Epoch 5/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 875us/step - accuracy: 0.9487 - loss: 0.2058 - val_accuracy: 0.9542 - val_loss: 0.1858\n",
      "Epoch 6/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 931us/step - accuracy: 0.9487 - loss: 0.2042 - val_accuracy: 0.9542 - val_loss: 0.1944\n",
      "Epoch 7/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 895us/step - accuracy: 0.9448 - loss: 0.2151 - val_accuracy: 0.9542 - val_loss: 0.1867\n",
      "Epoch 8/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 879us/step - accuracy: 0.9483 - loss: 0.2072 - val_accuracy: 0.9542 - val_loss: 0.1859\n",
      "Epoch 9/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 877us/step - accuracy: 0.9485 - loss: 0.2047 - val_accuracy: 0.9542 - val_loss: 0.1858\n",
      "Epoch 10/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 874us/step - accuracy: 0.9493 - loss: 0.2019 - val_accuracy: 0.9542 - val_loss: 0.1862\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 508us/step\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9381 - loss: 0.2282 - val_accuracy: 0.9465 - val_loss: 0.2101\n",
      "Epoch 2/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 911us/step - accuracy: 0.9507 - loss: 0.1976 - val_accuracy: 0.9465 - val_loss: 0.2087\n",
      "Epoch 3/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 872us/step - accuracy: 0.9515 - loss: 0.1965 - val_accuracy: 0.9465 - val_loss: 0.2087\n",
      "Epoch 4/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 879us/step - accuracy: 0.9495 - loss: 0.2018 - val_accuracy: 0.9465 - val_loss: 0.2110\n",
      "Epoch 5/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 877us/step - accuracy: 0.9530 - loss: 0.1917 - val_accuracy: 0.9465 - val_loss: 0.2091\n",
      "Epoch 6/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 877us/step - accuracy: 0.9522 - loss: 0.1947 - val_accuracy: 0.9465 - val_loss: 0.2099\n",
      "Epoch 7/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 869us/step - accuracy: 0.9482 - loss: 0.2076 - val_accuracy: 0.9465 - val_loss: 0.2230\n",
      "Epoch 8/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 873us/step - accuracy: 0.9507 - loss: 0.1986 - val_accuracy: 0.9465 - val_loss: 0.2087\n",
      "Epoch 9/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 887us/step - accuracy: 0.9494 - loss: 0.2021 - val_accuracy: 0.9465 - val_loss: 0.2088\n",
      "Epoch 10/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 904us/step - accuracy: 0.9515 - loss: 0.1962 - val_accuracy: 0.9465 - val_loss: 0.2132\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 444us/step\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "D:\\anaconda\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9508 - loss: 0.2185 - val_accuracy: 0.9525 - val_loss: 0.1920\n",
      "Epoch 2/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 908us/step - accuracy: 0.9503 - loss: 0.1991 - val_accuracy: 0.9525 - val_loss: 0.1913\n",
      "Epoch 3/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 884us/step - accuracy: 0.9532 - loss: 0.1902 - val_accuracy: 0.9525 - val_loss: 0.1944\n",
      "Epoch 4/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 870us/step - accuracy: 0.9481 - loss: 0.2057 - val_accuracy: 0.9525 - val_loss: 0.1912\n",
      "Epoch 5/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 870us/step - accuracy: 0.9487 - loss: 0.2040 - val_accuracy: 0.9525 - val_loss: 0.1912\n",
      "Epoch 6/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 878us/step - accuracy: 0.9483 - loss: 0.2053 - val_accuracy: 0.9525 - val_loss: 0.1935\n",
      "Epoch 7/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 871us/step - accuracy: 0.9482 - loss: 0.2051 - val_accuracy: 0.9525 - val_loss: 0.1948\n",
      "Epoch 8/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 875us/step - accuracy: 0.9483 - loss: 0.2078 - val_accuracy: 0.9525 - val_loss: 0.1913\n",
      "Epoch 9/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 899us/step - accuracy: 0.9467 - loss: 0.2098 - val_accuracy: 0.9525 - val_loss: 0.1928\n",
      "Epoch 10/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 879us/step - accuracy: 0.9474 - loss: 0.2081 - val_accuracy: 0.9525 - val_loss: 0.1947\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 460us/step\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9012 - loss: 0.2655 - val_accuracy: 0.9457 - val_loss: 0.2160\n",
      "Epoch 2/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 883us/step - accuracy: 0.9551 - loss: 0.1838 - val_accuracy: 0.9457 - val_loss: 0.2109\n",
      "Epoch 3/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 882us/step - accuracy: 0.9487 - loss: 0.2045 - val_accuracy: 0.9457 - val_loss: 0.2186\n",
      "Epoch 4/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 896us/step - accuracy: 0.9548 - loss: 0.1870 - val_accuracy: 0.9457 - val_loss: 0.2182\n",
      "Epoch 5/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 884us/step - accuracy: 0.9513 - loss: 0.1976 - val_accuracy: 0.9457 - val_loss: 0.2139\n",
      "Epoch 6/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 875us/step - accuracy: 0.9488 - loss: 0.2046 - val_accuracy: 0.9457 - val_loss: 0.2123\n",
      "Epoch 7/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 900us/step - accuracy: 0.9513 - loss: 0.1990 - val_accuracy: 0.9457 - val_loss: 0.2109\n",
      "Epoch 8/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 904us/step - accuracy: 0.9523 - loss: 0.1935 - val_accuracy: 0.9457 - val_loss: 0.2116\n",
      "Epoch 9/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 878us/step - accuracy: 0.9537 - loss: 0.1892 - val_accuracy: 0.9457 - val_loss: 0.2145\n",
      "Epoch 10/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 877us/step - accuracy: 0.9517 - loss: 0.1957 - val_accuracy: 0.9457 - val_loss: 0.2116\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 476us/step\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9273 - loss: 0.2338 - val_accuracy: 0.9510 - val_loss: 0.1957\n",
      "Epoch 2/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 909us/step - accuracy: 0.9494 - loss: 0.2043 - val_accuracy: 0.9510 - val_loss: 0.1986\n",
      "Epoch 3/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 914us/step - accuracy: 0.9475 - loss: 0.2083 - val_accuracy: 0.9510 - val_loss: 0.1959\n",
      "Epoch 4/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 910us/step - accuracy: 0.9501 - loss: 0.2002 - val_accuracy: 0.9510 - val_loss: 0.1974\n",
      "Epoch 5/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 918us/step - accuracy: 0.9505 - loss: 0.1984 - val_accuracy: 0.9510 - val_loss: 0.1968\n",
      "Epoch 6/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 905us/step - accuracy: 0.9510 - loss: 0.1976 - val_accuracy: 0.9510 - val_loss: 0.1970\n",
      "Epoch 7/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 879us/step - accuracy: 0.9515 - loss: 0.1955 - val_accuracy: 0.9510 - val_loss: 0.2026\n",
      "Epoch 8/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 866us/step - accuracy: 0.9471 - loss: 0.2116 - val_accuracy: 0.9510 - val_loss: 0.1988\n",
      "Epoch 9/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 887us/step - accuracy: 0.9503 - loss: 0.1995 - val_accuracy: 0.9510 - val_loss: 0.1956\n",
      "Epoch 10/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 882us/step - accuracy: 0.9500 - loss: 0.2019 - val_accuracy: 0.9510 - val_loss: 0.1960\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 468us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(columns=['Fold', 'ACC', 'Recall', 'F1-score', 'Precision', 'Savings'])\n",
    "y = train['欺诈状态']\n",
    "X = train.drop(columns=['欺诈状态'])\n",
    "\n",
    "# 5-fold cross-validation\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(X)):\n",
    "    \n",
    "    # Split the train and test dataset\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    \n",
    "    # Reshape\n",
    "    X_train = np.reshape(X_train.values, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "    X_val = np.reshape(X_val.values, (X_val.shape[0], 1, X_val.shape[1]))\n",
    "    \n",
    "    # Train model\n",
    "    model = tuner.hypermodel.build(best_hps)\n",
    "    history = model.fit(X_train, y_train, \n",
    "                       epochs=10, \n",
    "                       validation_data=(X_val, y_val), \n",
    "                       verbose=1)\n",
    "    \n",
    "    # Obtain the predictions\n",
    "    pred = (model.predict(X_val) > 0.05).astype(\"int32\").flatten()  \n",
    "    y_val = y_val.values  \n",
    "    \n",
    "    # Create the cost matrix\n",
    "    fee = pd.DataFrame()\n",
    "    fee['数额'] = train['总审批金额'].iloc[val_index]\n",
    "    amounts_val = fee['数额'].values\n",
    "    \n",
    "    n_samples = len(amounts_val)\n",
    "    cost_matrix_val = np.zeros((n_samples, 2, 2))\n",
    "    cost_matrix_val[:, 0, 0] = 0\n",
    "    cost_matrix_val[:, 0, 1] = amounts_val\n",
    "    cost_matrix_val[:, 1, 0] = 20000\n",
    "    cost_matrix_val[:, 1, 1] = 15000\n",
    "    \n",
    "    # Obtain the performance metrics\n",
    "    fold_results = {\n",
    "        'Fold': fold,\n",
    "        'ACC': round(metrics.accuracy_score(y_val, pred), 4),\n",
    "        'Recall': round(metrics.recall_score(y_val, pred), 4),\n",
    "        'F1-score': round(metrics.f1_score(y_val, pred), 4),\n",
    "        'Precision': round(metrics.precision_score(y_val, pred), 4),\n",
    "        'Savings': round(savings1(cost_matrix_val, y_val, pred), 4)\n",
    "    }\n",
    "    results_df.loc[len(results_df)] = fold_results\n",
    "    \n",
    "# Save results\n",
    "# results_df.to_csv('results-gru/gru10.csv', index=False, float_format='%.4f')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "374.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
