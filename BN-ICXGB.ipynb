{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324e627d-f968-4fd5-bde0-3fd4c3d0ecb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from scipy.special import expit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d117260-135e-4300-a413-cd464d1023a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature=pd.read_csv('finalfeatures.csv')\n",
    "amount=pd.read_csv('费用类相关特征2.csv')\n",
    "amount=amount.set_index('个人编码')\n",
    "amount=amount[['总审批金额']]\n",
    "feature=feature.set_index('个人编码')\n",
    "feature=feature.drop(columns=['总审批金额'])\n",
    "feature=pd.concat([feature,amount],axis=1)\n",
    "feature = feature.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf4368b-0700-400c-a095-ad2a92a17d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "bayesdata=pd.read_csv('bayesdata2.csv')\n",
    "bayesdata=bayesdata.set_index('个人编码')\n",
    "feature=pd.concat([feature,bayesdata],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f81c99-5d2f-48b0-ba82-d93ebcafef3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost-sensitive XGB\n",
    "\n",
    "class CSBoost:\n",
    "    def __init__(self, obj, lambda1=0, lambda2=0, learn_rate=0.01):\n",
    "        self.obj = obj\n",
    "        self.lambda1 = lambda1\n",
    "        self.lambda2 = lambda2\n",
    "        self.learn_rate=learn_rate\n",
    "        # alpha is l1, lambda is l2\n",
    "        params = {'random_state': 42, 'tree_method': 'exact', 'verbosity': 0, 'reg_alpha': lambda1,\n",
    "                  'reg_lambda': lambda2,'learning_rate': learn_rate}\n",
    "        if obj == 'ce' or obj == 'weightedce':\n",
    "            params['objective'] = 'binary:logistic'\n",
    "        elif obj == 'aec':\n",
    "            params['disable_default_eval_metric'] = True\n",
    "\n",
    "        self.params = params\n",
    "\n",
    "    def fit(self, x_train, y_train, x_val, y_val, cost_matrix_train=None, cost_matrix_val=None):\n",
    "        x_train = np.array(x_train)\n",
    "        y_train = np.array(y_train)\n",
    "        x_val = np.array(x_val)\n",
    "        y_val = np.array(y_val)\n",
    "        cost_matrix_train = np.array(cost_matrix_train)\n",
    "        cost_matrix_val = np.array(cost_matrix_val)\n",
    "        if self.obj == 'ce':\n",
    "            dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "            dval = xgb.DMatrix(x_val, label=y_val)\n",
    "\n",
    "            xgboost = xgb.train(params=self.params, dtrain=dtrain, num_boost_round=500, early_stopping_rounds=50,\n",
    "                            evals=[(dval, 'eval')], verbose_eval=False)\n",
    "\n",
    "        elif self.obj == 'weightedce':\n",
    "            misclass_costs = np.zeros(len(y_train))\n",
    "            misclass_costs[y_train == 0] = cost_matrix_train[:, 1, 0][y_train == 0]\n",
    "            misclass_costs[y_train == 1] = cost_matrix_train[:, 0, 1][y_train == 1]\n",
    "\n",
    "            misclass_costs_val = np.zeros(len(y_val))\n",
    "            misclass_costs_val[y_val == 0] = cost_matrix_val[:, 1, 0][y_val == 0]\n",
    "            misclass_costs_val[y_val == 1] = cost_matrix_val[:, 0, 1][y_val == 1]\n",
    "\n",
    "            dtrain = xgb.DMatrix(x_train, label=y_train, weight=misclass_costs)\n",
    "            dval = xgb.DMatrix(x_val, label=y_val, weight=misclass_costs_val)\n",
    "\n",
    "            xgboost = xgb.train(params=self.params, dtrain=dtrain, num_boost_round=500, early_stopping_rounds=50,\n",
    "                                evals=[(dval, 'eval')], verbose_eval=False)\n",
    "\n",
    "        elif self.obj == 'aec':\n",
    "            dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "            dval = xgb.DMatrix(x_val, label=y_val)\n",
    "\n",
    "            # Do constant computations here to avoid DMatrix error\n",
    "            # diff_costs_train = fixed_cost - y_train * amounts_train\n",
    "\n",
    "            train_constant = (y_train * (cost_matrix_train[:, 1, 1] - cost_matrix_train[:, 0, 1])\n",
    "                              + (1 - y_train) * (cost_matrix_train[:, 1, 0] - cost_matrix_train[:, 0, 0]))\n",
    "\n",
    "            def aec_train(raw_scores, y_true):\n",
    "                scores = expit(raw_scores)\n",
    "\n",
    "                # Average expected cost:\n",
    "                # ec = np.multiply(np.multiply(y_true, (1 - scores)), amounts_train) + np.multiply(scores, fixed_cost)\n",
    "                # ec = y_true * (\n",
    "                #     scores * cost_matrix_train[:, 1, 1] + (1 - scores) * cost_matrix_val[:, 0, 1]) \\\n",
    "                #     + (1 - y_true) * (\n",
    "                #     scores * cost_matrix_val[:, 1, 0] + (1 - scores) * cost_matrix_val[:, 0, 0])\n",
    "\n",
    "                # Gradient\n",
    "                # Use diff_costs_train instead of (fixed_cost - y_true*amounts_train)\n",
    "                # grad = scores * (1 - scores) * diff_costs_train\n",
    "                grad = scores * (1 - scores) * train_constant\n",
    "\n",
    "                # Hessian\n",
    "                hess = np.abs((1 - 2 * scores) * grad)\n",
    "                # hess = scores * (1 - scores) * (1 - 2 * scores) * train_constant\n",
    "\n",
    "                # Grad and hess cannot be too close to 0!\n",
    "                # print(grad.mean())\n",
    "                # print(hess.mean())\n",
    "\n",
    "                return grad, hess\n",
    "\n",
    "            def aec_val(raw_scores, y_true):\n",
    "                scores = expit(raw_scores)\n",
    "\n",
    "                # Return AEC (not grad/hess)\n",
    "                # ec = (1 - scores) * y_val * amounts_val + scores * fixed_cost\n",
    "                # ec = y_true * (\n",
    "                #     scores * cost_matrix_val[:, 1, 1] + (1 - scores) * cost_matrix_val[:, 0, 1]) \\\n",
    "                #     + (1 - y_true) * (\n",
    "                #     scores * cost_matrix_val[:, 1, 0] + (1 - scores) * cost_matrix_val[:, 0, 0])\n",
    "\n",
    "                # Avoid computations with y_true (DMatrix)\n",
    "                if y_true:\n",
    "                    ec = scores * cost_matrix_val[:, 1, 1] + (1 - scores) * cost_matrix_val[:, 0, 1]\n",
    "                else:\n",
    "                    ec = scores * cost_matrix_val[:, 1, 0] + (1 - scores) * cost_matrix_val[:, 0, 0]\n",
    "\n",
    "                aec = ec.mean()\n",
    "\n",
    "                return 'AEC', aec\n",
    "\n",
    "            xgboost = xgb.train(params=self.params, dtrain=dtrain, obj=aec_train, feval=aec_val, num_boost_round=500,\n",
    "                                early_stopping_rounds=50, evals=[(dval, 'eval')], verbose_eval=False)\n",
    "\n",
    "        # print('\\tBest number of trees = %i' % xgboost.best_ntree_limit)\n",
    "\n",
    "        return xgboost\n",
    "\n",
    "    def tune(self, l1, lambda1_list, l2, lambda2_list, learn_rate, learn_ratelist, x_train, y_train, cost_matrix_train, x_val, y_val, cost_matrix_val):\n",
    "        if l1:\n",
    "            self.params['reg_lambda'] = 0\n",
    "            losses_list = []\n",
    "            for lambda1 in lambda1_list:\n",
    "                xgboost = CSBoost(obj=self.obj, lambda1=lambda1)\n",
    "                xgboost = xgboost.fit(x_train, y_train, x_val, y_val, cost_matrix_train, cost_matrix_val)\n",
    "                scores = xgboost.inplace_predict(x_val)\n",
    "\n",
    "                # Evaluate loss (without regularization term!)\n",
    "                if self.obj == 'ce':\n",
    "                    eps = 1e-9  # small value to avoid log(0)\n",
    "                    ce = - (y_val * np.log(scores + eps) + (1 - y_val) * np.log(1 - scores + eps))\n",
    "                    val_loss = ce.mean()\n",
    "                elif self.obj == 'weightedce':\n",
    "                    eps = 1e-9  # small value to avoid log(0)\n",
    "                    ce = - (y_val * np.log(scores + eps) + (1 - y_val) * np.log(1 - scores + eps))\n",
    "\n",
    "                    cost_misclass = np.zeros(len(y_val))\n",
    "                    cost_misclass[y_val == 0] = cost_matrix_val[:, 1, 0][y_val == 0]\n",
    "                    cost_misclass[y_val == 1] = cost_matrix_val[:, 0, 1][y_val == 1]\n",
    "\n",
    "                    weighted_ce = cost_misclass * ce\n",
    "                    val_loss = weighted_ce.mean()\n",
    "                elif self.obj == 'aec':\n",
    "                    def aec_val(raw_scores, y_true):\n",
    "                        scores = expit(raw_scores)\n",
    "\n",
    "                        # Return AEC (not grad/hess)\n",
    "                        # ec = (1 - scores) * y_val * amounts_val + scores * fixed_cost\n",
    "                        ec = y_true * (\n",
    "                            scores * cost_matrix_val[:, 1, 1] + (1 - scores) * cost_matrix_val[:, 0, 1]) \\\n",
    "                            + (1 - y_true) * (\n",
    "                            scores * cost_matrix_val[:, 1, 0] + (1 - scores) * cost_matrix_val[:, 0, 0])\n",
    "\n",
    "                        aec = ec.mean()\n",
    "\n",
    "                        return 'AEC', aec\n",
    "\n",
    "                    aec = aec_val(scores, y_val)\n",
    "                    val_loss = aec[1]\n",
    "                print('\\t\\tLambda l1 = %.5f;\\tLoss = %.5f' % (lambda1, val_loss))\n",
    "                losses_list.append(val_loss)\n",
    "            lambda1_opt = lambda1_list[np.argmin(losses_list)]\n",
    "            print('\\tOptimal lambda = %.5f' % lambda1_opt)\n",
    "            self.params['reg_alpha'] = lambda1_opt\n",
    "        elif l2:\n",
    "            self.params['reg_alpha'] = 0\n",
    "            losses_list = []\n",
    "            for lambda2 in lambda2_list:\n",
    "                xgboost = CSBoost(obj=self.obj, lambda2=lambda2)\n",
    "                xgboost = xgboost.fit(x_train, y_train, x_val, y_val, cost_matrix_train, cost_matrix_val)\n",
    "                scores = xgboost.inplace_predict(x_val)\n",
    "\n",
    "                # Evaluate loss (without regularization term!)\n",
    "                if self.obj == 'ce':\n",
    "                    eps = 1e-9  # small value to avoid log(0)\n",
    "                    ce = - (y_val * np.log(scores + eps) + (1 - y_val) * np.log(1 - scores + eps))\n",
    "                    val_loss = ce.mean()\n",
    "                elif self.obj == 'weightedce':\n",
    "                    eps = 1e-9  # small value to avoid log(0)\n",
    "                    ce = - (y_val * np.log(scores + eps) + (1 - y_val) * np.log(1 - scores + eps))\n",
    "\n",
    "                    cost_misclass = np.zeros(len(y_val))\n",
    "                    cost_misclass[y_val == 0] = cost_matrix_val[:, 1, 0][y_val == 0]\n",
    "                    cost_misclass[y_val == 1] = cost_matrix_val[:, 0, 1][y_val == 1]\n",
    "\n",
    "                    weighted_ce = cost_misclass * ce\n",
    "                    val_loss = weighted_ce.mean()\n",
    "                elif self.obj == 'aec':\n",
    "                    def aec_val(raw_scores, y_true):\n",
    "                        scores = expit(raw_scores)\n",
    "\n",
    "                        # Return AEC (not grad/hess)\n",
    "                        ec = y_true * (\n",
    "                                scores * cost_matrix_val[:, 1, 1] + (1 - scores) * cost_matrix_val[:, 0, 1]) \\\n",
    "                             + (1 - y_true) * (\n",
    "                                     scores * cost_matrix_val[:, 1, 0] + (1 - scores) * cost_matrix_val[:, 0, 0])\n",
    "\n",
    "                        aec = ec.mean()\n",
    "\n",
    "                        return 'AEC', aec\n",
    "\n",
    "                    aec = aec_val(scores, y_val)\n",
    "                    val_loss = aec[1]\n",
    "                print('\\t\\tLambda l2 = %.5f;\\tLoss = %.5f' % (lambda2, val_loss))\n",
    "                losses_list.append(val_loss)\n",
    "            lambda2_opt = lambda2_list[np.argmin(losses_list)]\n",
    "            print('\\tOptimal lambda = %.5f' % lambda2_opt)\n",
    "            self.params['reg_alpha'] = lambda2_opt\n",
    "        elif learn_rate:\n",
    "            self.params['learning_rate'] = 0.01\n",
    "            losses_list = []\n",
    "            for learn_rate in learn_ratelist:\n",
    "                xgboost = CSBoost(obj=self.obj, learn_rate=learn_rate)\n",
    "                xgboost = xgboost.fit(x_train, y_train, x_val, y_val, cost_matrix_train, cost_matrix_val)\n",
    "                scores = xgboost.inplace_predict(x_val)\n",
    "\n",
    "                # Evaluate loss (without regularization term!)\n",
    "                if self.obj == 'ce':\n",
    "                    eps = 1e-9  # small value to avoid log(0)\n",
    "                    ce = - (y_val * np.log(scores + eps) + (1 - y_val) * np.log(1 - scores + eps))\n",
    "                    val_loss = ce.mean()\n",
    "                elif self.obj == 'weightedce':\n",
    "                    eps = 1e-9  # small value to avoid log(0)\n",
    "                    ce = - (y_val * np.log(scores + eps) + (1 - y_val) * np.log(1 - scores + eps))\n",
    "\n",
    "                    cost_misclass = np.zeros(len(y_val))\n",
    "                    cost_misclass[y_val == 0] = cost_matrix_val[:, 1, 0][y_val == 0]\n",
    "                    cost_misclass[y_val == 1] = cost_matrix_val[:, 0, 1][y_val == 1]\n",
    "\n",
    "                    weighted_ce = cost_misclass * ce\n",
    "                    val_loss = weighted_ce.mean()\n",
    "                elif self.obj == 'aec':\n",
    "                    def aec_val(raw_scores, y_true):\n",
    "                        scores = expit(raw_scores)\n",
    "\n",
    "                        # Return AEC (not grad/hess)\n",
    "                        # ec = (1 - scores) * y_val * amounts_val + scores * fixed_cost\n",
    "                        ec = y_true * (\n",
    "                            scores * cost_matrix_val[:, 1, 1] + (1 - scores) * cost_matrix_val[:, 0, 1]) \\\n",
    "                            + (1 - y_true) * (\n",
    "                            scores * cost_matrix_val[:, 1, 0] + (1 - scores) * cost_matrix_val[:, 0, 0])\n",
    "\n",
    "                        aec = ec.mean()\n",
    "\n",
    "                        return 'AEC', aec\n",
    "\n",
    "                    aec = aec_val(scores, y_val)\n",
    "                    val_loss = aec[1]\n",
    "                print('\\t\\tlearning_rate = %.5f;\\tLoss = %.5f' % (learn_rate, val_loss))\n",
    "                losses_list.append(val_loss)\n",
    "            learn_rate_opt = learn_ratelist[np.argmin(losses_list)]\n",
    "            print('\\tOptimal learn_rate = %.5f' % learn_rate_opt)\n",
    "            self.params['learn_rate'] = learn_rate_opt\n",
    "        else:\n",
    "            self.lambda1 = 0\n",
    "            self.lambda2 = 0\n",
    "            self.learn_rate=0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c4f48a-b732-402f-8b63-bebaa1a744c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search for hyperparameters\n",
    "\n",
    "y_train=feature['欺诈状态']\n",
    "x_train=feature.drop(columns=['欺诈状态'])\n",
    "\n",
    "#Split the train set and test set\n",
    "x_train, x_val, y_train, y_val=train_test_split(x_train, y_train, test_size=0.2, shuffle=True, random_state=333)\n",
    "\n",
    "#Create cost matrix\n",
    "amounts_train = x_train['总审批金额'].values\n",
    "amounts_val = x_val['总审批金额'].values\n",
    "\n",
    "cost_matrix_train = np.zeros((len(x_train), 2, 2))     # cost_matrix [[TN, FN], [FP, TP]]\n",
    "cost_matrix_train[:, 0, 0] = 0.0\n",
    "cost_matrix_train[:, 0, 1] = amounts_train\n",
    "cost_matrix_train[:, 1, 0] = 20000\n",
    "cost_matrix_train[:, 1, 1] = 15000\n",
    "\n",
    "cost_matrix_val = np.zeros((len(x_val), 2, 2))     # cost_matrix [[TN, FN], [FP, TP]]\n",
    "cost_matrix_val[:, 0, 0] = 0.0\n",
    "cost_matrix_val[:, 0, 1] = amounts_val\n",
    "cost_matrix_val[:, 1, 0] = 20000\n",
    "cost_matrix_val[:, 1, 1] = 15000\n",
    "\n",
    "#Obtain hyper-parameters\n",
    "csboost = CSBoost(obj='aec')\n",
    "csboost.tune(False, [0, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1], False, [0, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1], True,[0.01,0.02,0.05,0.1,0.15], x_train, y_train,cost_matrix_train, x_val, y_val, cost_matrix_val)\n",
    "# lambda1 = 0.01\n",
    "# lambda2 = 0.00001\n",
    "# learn_rate=0.1\n",
    "#csboost=csboost.fit(x_train, y_train, x_val, y_val, cost_matrix_train, cost_matrix_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2999168-3bc4-4559-a3dc-39cf3524fb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_with_algorithm(cost_matrix, labels, predictions):\n",
    "\n",
    "    cost_tn = cost_matrix[:, 0, 0][np.logical_and(predictions == 0, labels == 0)].sum()\n",
    "    cost_fn = cost_matrix[:, 0, 1][np.logical_and(predictions == 0, labels == 1)].sum()\n",
    "    cost_fp = cost_matrix[:, 1, 0][np.logical_and(predictions == 1, labels == 0)].sum()\n",
    "    cost_tp = cost_matrix[:, 1, 1][np.logical_and(predictions == 1, labels == 1)].sum()\n",
    "\n",
    "    return sum((cost_tn, cost_fn, cost_fp, cost_tp))\n",
    "\n",
    "def cost_without_algorithm(cost_matrix, labels):\n",
    "\n",
    "    # Predict everything as the default class that leads to minimal cost\n",
    "    # Also include cost of TP/TN!\n",
    "    cost_neg = cost_matrix[:, 0, 0][labels == 0].sum() + cost_matrix[:, 0, 1][labels == 1].sum()\n",
    "    cost_pos = cost_matrix[:, 1, 0][labels == 0].sum() + cost_matrix[:, 1, 1][labels == 1].sum()\n",
    "\n",
    "    return min(cost_neg, cost_pos)\n",
    "\n",
    "def savings(cost_matrix, labels, predictions):\n",
    "    \n",
    "    cost_without = cost_without_algorithm(cost_matrix, labels)\n",
    "    cost_with = cost_with_algorithm(cost_matrix, labels, predictions)\n",
    "    savings = 1 - cost_with / cost_without\n",
    "    \n",
    "    return savings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9255a03e-c00f-4ea4-ac04-84a0e19df7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "train = feature\n",
    "y = train['欺诈状态']\n",
    "X = train.drop(columns=['欺诈状态'])\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(columns=['Fold', 'Recall', 'F1-score', 'Precision', 'Savings'])\n",
    "\n",
    "# Create a 5-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=1000)\n",
    "\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "    \n",
    "    # Split the dataset\n",
    "    x_train, x_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "    # Construct the cost matrix\n",
    "    amounts_train = x_train['总审批金额'].values\n",
    "    amounts_val = x_val['总审批金额'].values\n",
    "    \n",
    "    cost_matrix_train = np.zeros((len(x_train), 2, 2))\n",
    "    cost_matrix_train[:, 0, 0] = 0\n",
    "    cost_matrix_train[:, 0, 1] = amounts_train\n",
    "    cost_matrix_train[:, 1, 0] = 20000\n",
    "    cost_matrix_train[:, 1, 1] = 15000\n",
    "    \n",
    "    cost_matrix_val = np.zeros((len(x_val), 2, 2))\n",
    "    cost_matrix_val[:, 0, 0] = 0\n",
    "    cost_matrix_val[:, 0, 1] = amounts_val\n",
    "    cost_matrix_val[:, 1, 0] = 20000\n",
    "    cost_matrix_val[:, 1, 1] = 15000\n",
    "    \n",
    "    # Train the model\n",
    "    csboost = CSBoost(obj='aec')\n",
    "    lambda1 = 0.1\n",
    "    lambda2 = 0.001\n",
    "    learn_rate=0.15\n",
    "    csboost = csboost.fit(x_train, y_train, x_val, y_val, cost_matrix_train, cost_matrix_val)\n",
    "    \n",
    "    # Obtain the predictions\n",
    "    pre_val = expit(csboost.inplace_predict(x_val))\n",
    "    threshold_instance = (cost_matrix_val[:, 1, 0] - cost_matrix_val[:, 0, 0]) / (\n",
    "        cost_matrix_val[:, 1, 0] - cost_matrix_val[:, 0, 0]\n",
    "        + cost_matrix_val[:, 0, 1] - cost_matrix_val[:, 1, 1])\n",
    "    pred = (pre_val > threshold_instance).astype(int)\n",
    "    \n",
    "    # Get the performance metrics\n",
    "    fold_results = {\n",
    "        'Fold': fold,\n",
    "        'ACC': round(metrics.accuracy_score(y_val, pred), 4),\n",
    "        'Recall': round(metrics.recall_score(y_val, pred), 4),\n",
    "        'F1-score': round(metrics.f1_score(y_val, pred), 4),\n",
    "        'Precision': round(metrics.precision_score(y_val, pred), 4),\n",
    "        'Savings': round(savings(cost_matrix_val, y_val, pred), 4)\n",
    "    }\n",
    "    \n",
    "    results_df = pd.concat([results_df, pd.DataFrame([fold_results])], ignore_index=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
